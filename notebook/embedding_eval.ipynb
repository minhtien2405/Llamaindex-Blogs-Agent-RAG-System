{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Study\\\\VietAI\\\\CV_Chatbot\\\\notebook'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/llama_blog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/blog/introducing-llamaindex-0-11</td>\n",
       "      <td>Introducing LlamaIndex 0.11</td>\n",
       "      <td>https://www.llamaindex.ai/blog/introducing-lla...</td>\n",
       "      <td>Aug 22, 2024</td>\n",
       "      <td>LlamaIndex is delighted to announce that we ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/blog/efficient-chunk-size-optimization-for-ra...</td>\n",
       "      <td>Efficient Chunk Size Optimization for RAG Pipe...</td>\n",
       "      <td>https://www.llamaindex.ai/blog/efficient-chunk...</td>\n",
       "      <td>Aug 21, 2024</td>\n",
       "      <td>In Retrieval-Augmented Generation (RAG) system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/blog/llamaindex-newsletter-2024-08-20</td>\n",
       "      <td>LlamaIndex Newsletter 2024-08-20</td>\n",
       "      <td>https://www.llamaindex.ai/blog/llamaindex-news...</td>\n",
       "      <td>Aug 20, 2024</td>\n",
       "      <td>Hi there, Llama Lovers! ðŸ¦™\\n\\nWelcome to this w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/blog/llamaindex-newsletter-2024-08-13</td>\n",
       "      <td>LlamaIndex Newsletter 2024-08-13</td>\n",
       "      <td>https://www.llamaindex.ai/blog/llamaindex-news...</td>\n",
       "      <td>Aug 13, 2024</td>\n",
       "      <td>Hi there, Llama Fans! ðŸ¦™\\n\\nWelcome to this wee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/blog/llamaindex-newsletter-2024-08-06</td>\n",
       "      <td>LlamaIndex Newsletter 2024-08-06</td>\n",
       "      <td>https://www.llamaindex.ai/blog/llamaindex-news...</td>\n",
       "      <td>Aug 6, 2024</td>\n",
       "      <td>Greetings, Llama Lovers! ðŸ¦™\\n\\nWelcome to this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0                  /blog/introducing-llamaindex-0-11   \n",
       "1  /blog/efficient-chunk-size-optimization-for-ra...   \n",
       "2             /blog/llamaindex-newsletter-2024-08-20   \n",
       "3             /blog/llamaindex-newsletter-2024-08-13   \n",
       "4             /blog/llamaindex-newsletter-2024-08-06   \n",
       "\n",
       "                                               title  \\\n",
       "0                        Introducing LlamaIndex 0.11   \n",
       "1  Efficient Chunk Size Optimization for RAG Pipe...   \n",
       "2                   LlamaIndex Newsletter 2024-08-20   \n",
       "3                   LlamaIndex Newsletter 2024-08-13   \n",
       "4                   LlamaIndex Newsletter 2024-08-06   \n",
       "\n",
       "                                                 url          date  \\\n",
       "0  https://www.llamaindex.ai/blog/introducing-lla...  Aug 22, 2024   \n",
       "1  https://www.llamaindex.ai/blog/efficient-chunk...  Aug 21, 2024   \n",
       "2  https://www.llamaindex.ai/blog/llamaindex-news...  Aug 20, 2024   \n",
       "3  https://www.llamaindex.ai/blog/llamaindex-news...  Aug 13, 2024   \n",
       "4  https://www.llamaindex.ai/blog/llamaindex-news...   Aug 6, 2024   \n",
       "\n",
       "                                             content  \n",
       "0  LlamaIndex is delighted to announce that we ha...  \n",
       "1  In Retrieval-Augmented Generation (RAG) system...  \n",
       "2  Hi there, Llama Lovers! ðŸ¦™\\n\\nWelcome to this w...  \n",
       "3  Hi there, Llama Fans! ðŸ¦™\\n\\nWelcome to this wee...  \n",
       "4  Greetings, Llama Lovers! ðŸ¦™\\n\\nWelcome to this ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "# Clean the text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "data['title'] = data['title'].apply(clean_text)\n",
    "data['content'] = data['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, Document, VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "# Initialize the OpenAI  embedding\n",
    "Settings.embedding = OpenAIEmbedding(model = \"text-embedding-3-small\")\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        text=row['content'],\n",
    "        metadata={\n",
    "            'source': row['source'],\n",
    "            'title': row['title'],\n",
    "            'url': row['url'],\n",
    "            'date': row['date'],\n",
    "        },\n",
    "    )\n",
    "    for index, row in data.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 256\n",
      "Chunk Size: 512\n",
      "Chunk Size: 1024\n",
      "Chunk Size: 2048\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings, Document, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "chunk_sizes = [256, 512, 1024, 2048]\n",
    "nodes_list = []\n",
    "vector_indices = []\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"Chunk Size: {chunk_size}\")\n",
    "    nodes = SentenceSplitter(chunk_size=chunk_size,\n",
    "                                chunk_overlap=128,\n",
    "                                paragraph_separator=\"\\n\\n\",\n",
    "                            ).get_nodes_from_documents(documents)\n",
    "\n",
    "    # add chunk size to nodes to track later\n",
    "    for node in nodes:\n",
    "        node.metadata[\"chunk_size\"] = chunk_size\n",
    "        node.excluded_embed_metadata_keys = [\"chunk_size\"]\n",
    "        node.excluded_llm_metadata_keys = [\"chunk_size\"]\n",
    "\n",
    "    nodes_list.append(nodes)\n",
    "\n",
    "    # build vector index\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    vector_indices.append(vector_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "Settings.llm = OpenAI(model = \"gpt-4o\")\n",
    "Settings.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import RetrieverTool\n",
    "from llama_index.core.schema import IndexNode\n",
    "\n",
    "# retriever_tools = []\n",
    "retriever_dict = {}\n",
    "retriever_nodes = []\n",
    "for chunk_size, vector_index in zip(chunk_sizes, vector_indices):\n",
    "    node_id = f\"chunk_{chunk_size}\"\n",
    "    node = IndexNode(\n",
    "        text=(\n",
    "            \"Retrieves relevant context from the all Llama Blog posts. chunk_size: \"\n",
    "            f\" {chunk_size})\"\n",
    "        ),\n",
    "        index_id=node_id,\n",
    "    )\n",
    "    retriever_nodes.append(node)\n",
    "    retriever_dict[node_id] = vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.selectors import PydanticMultiSelector\n",
    "\n",
    "from llama_index.core.retrievers import RouterRetriever\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core import SummaryIndex\n",
    "\n",
    "# the derived retriever will just retrieve all nodes\n",
    "summary_index = SummaryIndex(retriever_nodes)\n",
    "\n",
    "retriever = RecursiveRetriever(\n",
    "    root_id=\"root\",\n",
    "    retriever_dict={\"root\": summary_index.as_retriever(), **retriever_dict},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define reranker\n",
    "from llama_index.core.postprocessor import LLMRerank, SentenceTransformerRerank\n",
    "\n",
    "reranker = LLMRerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine  = RetrieverQueryEngine(retriever, node_postprocessors=[reranker])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_response\n",
    "display_response(\n",
    "    response, show_source=True, source_length=500, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def mrr_all(metadata_values, metadata_key, source_nodes):\n",
    "    # source nodes is a ranked list\n",
    "    # go through each value, find out positioning in source_nodes\n",
    "    value_to_mrr_dict = {}\n",
    "    for metadata_value in metadata_values:\n",
    "        mrr = 0\n",
    "        for idx, source_node in enumerate(source_nodes):\n",
    "            if source_node.node.metadata[metadata_key] == metadata_value:\n",
    "                mrr = 1 / (idx + 1)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # normalize AP, set in dict\n",
    "        value_to_mrr_dict[metadata_value] = mrr\n",
    "\n",
    "    df = pd.DataFrame(value_to_mrr_dict, index=[\"MRR\"])\n",
    "    df.style.set_caption(\"Mean Reciprocal Rank\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank for each Chunk Size\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>256</th>\n",
       "      <th>512</th>\n",
       "      <th>1024</th>\n",
       "      <th>2048</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MRR</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     256       512   1024      2048\n",
       "MRR   1.0  0.333333   0.2  0.166667"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean Reciprocal Rank for each Chunk Size\")\n",
    "mrr_all(chunk_sizes, \"chunk_size\", response.source_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
