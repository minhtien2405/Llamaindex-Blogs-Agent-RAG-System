{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import requests\n",
    "import getpass\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import gradio as gr\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.core import Settings, Document, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.agent import ReActAgent, FunctionCallingAgentWorker, AgentRunner\n",
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate, Settings\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for OpenAI\nlogprobs\n  Field required [type=missing, input_value={'model': 'gpt-4o', 'temp...y': 0.0, 'stop': ['\\n']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.7/v/missing\ndefault_headers\n  Input should be a valid dictionary [type=dict_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.7/v/dict_type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Settings\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\minht\\anaconda3\\envs\\nlp\\lib\\site-packages\\llama_index\\llms\\openai\\base.py:242\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, model, temperature, max_tokens, additional_kwargs, max_retries, timeout, reuse_client, api_key, api_base, api_version, callback_manager, default_headers, http_client, async_http_client, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m additional_kwargs \u001b[38;5;241m=\u001b[39m additional_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    236\u001b[0m api_key, api_base, api_version \u001b[38;5;241m=\u001b[39m resolve_openai_credentials(\n\u001b[0;32m    237\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m    238\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[0;32m    239\u001b[0m     api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[0;32m    240\u001b[0m )\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    243\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    244\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    245\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m    246\u001b[0m     additional_kwargs\u001b[38;5;241m=\u001b[39madditional_kwargs,\n\u001b[0;32m    247\u001b[0m     max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[0;32m    248\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m    249\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m    250\u001b[0m     api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[0;32m    251\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[0;32m    252\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    253\u001b[0m     reuse_client\u001b[38;5;241m=\u001b[39mreuse_client,\n\u001b[0;32m    254\u001b[0m     default_headers\u001b[38;5;241m=\u001b[39mdefault_headers,\n\u001b[0;32m    255\u001b[0m     system_prompt\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[0;32m    256\u001b[0m     messages_to_prompt\u001b[38;5;241m=\u001b[39mmessages_to_prompt,\n\u001b[0;32m    257\u001b[0m     completion_to_prompt\u001b[38;5;241m=\u001b[39mcompletion_to_prompt,\n\u001b[0;32m    258\u001b[0m     pydantic_program_mode\u001b[38;5;241m=\u001b[39mpydantic_program_mode,\n\u001b[0;32m    259\u001b[0m     output_parser\u001b[38;5;241m=\u001b[39moutput_parser,\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    261\u001b[0m )\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\minht\\anaconda3\\envs\\nlp\\lib\\site-packages\\pydantic\\main.py:176\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    175\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: 2 validation errors for OpenAI\nlogprobs\n  Field required [type=missing, input_value={'model': 'gpt-4o', 'temp...y': 0.0, 'stop': ['\\n']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.7/v/missing\ndefault_headers\n  Input should be a valid dictionary [type=dict_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.7/v/dict_type"
     ]
    }
   ],
   "source": [
    "Settings.llm = OpenAI(model=\"gpt-4o\", \n",
    "                        max_tokens=1000, \n",
    "                        temperature=0.5, \n",
    "                        top_p=1.0, \n",
    "                        frequency_penalty=0.0, \n",
    "                        presence_penalty=0.0, \n",
    "                        stop=[\"\\n\"]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    host=os.getenv('QDRANT_HOST'),\n",
    "    port=os.getenv('QDRANT_PORT'),\n",
    "    api_key=os.getenv('QDRANT_API_KEY'),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=os.getenv('QDRANT_COLLECTION_NAME'),\n",
    ")\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "ranker = LLMRerank(\n",
    "            choice_batch_size=5, top_n=3, llm=OpenAI(model=\"gpt-4o-mini\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm = Settings.llm,\n",
    "    node_post_processor=[ranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: /blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\n",
      "Title: Boosting RAG: Picking the Best Embedding & Reranker models\n",
      "URL: https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\n",
      "Date: Nov 3, 2023\n",
      "Elapsed: 2.3s\n"
     ]
    }
   ],
   "source": [
    "# Defining the self RAG logic flow\n",
    "now = time()\n",
    "response = query_engine.query(\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\")\n",
    "display_response(response)\n",
    "\n",
    "print(f\"Source: {response.source_nodes[0].metadata['source']}\")\n",
    "print(f\"Title: {response.source_nodes[0].metadata['title']}\")\n",
    "print(f\"URL: {response.source_nodes[0].metadata['url']}\")\n",
    "print(f\"Date: {response.source_nodes[0].metadata['date']}\")\n",
    "print(f\"Elapsed: {round(time() - now, 2)}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 4bde7f43-606f-47db-b581-bb338b60efe4<br>**Similarity:** 0.88150674<br>**Text:** Let's dive in! Retrieval Evaluation Let’s first start with understanding the metrics available in Retrieval Evaluation Retrieval Evaluation Section: Understanding Metrics in Retrieval Evaluation:: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics: Hit Rate and Mean Reciprocal Rank (MRR). Let’s delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct an...<br>**Metadata:** {'source': '/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 2d0a3c89-1155-45a4-b2e0-12e0fd338c7d<br>**Similarity:** 0.86984634<br>**Text:** How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we’ll use the Retrieval Evaluation module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Retrieval Evaluation Let’s first start with understanding the metrics available in Retrieval Evaluation Retrieval Evaluation Section: Understanding Metrics in Retrieval Evaluation:: To gauge the efficacy of our retrieval syst...<br>**Metadata:** {'source': '/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(\n",
    "    response, show_source=True, source_length=500, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode = \"openai\",\n",
    "    llm = Settings.llm,\n",
    "    node_post_processor=[ranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Hello again! How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Hi\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"What is llama-index?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** LlamaIndex is an advanced data framework tailored for large language model (LLM) applications. It offers a comprehensive suite of features for managing and querying data, such as:\n",
       "\n",
       "- **Data Ingestion**: Importing data from various sources.\n",
       "- **Parsing/Slicing**: Processing and structuring data.\n",
       "- **Storage/Indexing**: Efficiently storing and indexing data for quick retrieval.\n",
       "- **Retrieval**: Accessing the stored data as needed.\n",
       "- **Response Synthesis**: Generating responses based on the retrieved data.\n",
       "- **Multi-step Interactions**: Handling complex queries that require multiple steps.\n",
       "\n",
       "LlamaIndex facilitates the integration of individual or enterprise data from diverse sources, including files, workplace applications, and databases, with LLM applications. It also supports numerous integrations with storage providers, downstream applications, and observability and experimentation frameworks. Additionally, it can function as a ChatGPT Retrieval Plugin or be used with Poe."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The two main metrics used to evaluate the performance of the different rerankers in the RAG (Retrieval-Augmented Generation) system are:\n",
       "\n",
       "1. **Hit Rate**: This measures the frequency at which the correct answer appears in the top-k retrieved documents.\n",
       "2. **Mean Reciprocal Rank (MRR)**: This metric evaluates the rank of the first correct answer by taking the reciprocal of its rank and averaging these values across multiple queries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 4bde7f43-606f-47db-b581-bb338b60efe4<br>**Similarity:** 0.88150674<br>**Text:** Let's dive in! Retrieval Evaluation Let’s first start with understanding the metrics available in Retrieval Evaluation Retrieval Evaluation Section: Understanding Metrics in Retrieval Evaluation:: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics: Hit Rate and Mean Reciprocal Rank (MRR). Let’s delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct an...<br>**Metadata:** {'source': '/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 2d0a3c89-1155-45a4-b2e0-12e0fd338c7d<br>**Similarity:** 0.86984634<br>**Text:** How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we’ll use the Retrieval Evaluation module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Retrieval Evaluation Let’s first start with understanding the metrics available in Retrieval Evaluation Retrieval Evaluation Section: Understanding Metrics in Retrieval Evaluation:: To gauge the efficacy of our retrieval syst...<br>**Metadata:** {'source': '/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(\n",
    "    response, show_source=True, source_length=500, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The best way to evaluate the performance of different rerankers in the RAG (Retrieval-Augmented Generation) system is to use the Retrieval Evaluation module from LlamaIndex and focus on the following two key metrics:\n",
       "\n",
       "1. **Hit Rate**: This metric measures the fraction of queries where the correct answer is found within the top-k retrieved documents. It indicates how often the system retrieves the correct document within the top guesses.\n",
       "\n",
       "2. **Mean Reciprocal Rank (MRR)**: This metric assesses the system's accuracy by considering the rank of the highest-placed relevant document for each query. It takes the reciprocal of the rank of the first relevant document and averages these values across multiple queries.\n",
       "\n",
       "By focusing on these metrics, you can gain a comprehensive understanding of the rerankers' effectiveness in retrieving relevant documents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 2d0a3c89-1155-45a4-b2e0-12e0fd338c7d<br>**Similarity:** 0.86747634<br>**Text:** How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we’ll use the Retrieval Evaluation module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Retrieval Evaluation Let’s first start with understanding the metrics available in Retrieval Evaluation Retrieval Evaluation Section: Understanding Metrics in Retrieval Evaluation:: To gauge the efficacy of our retrieval syst...<br>**Metadata:** {'source': '/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 4bde7f43-606f-47db-b581-bb338b60efe4<br>**Similarity:** 0.8631115<br>**Text:** Let's dive in! Retrieval Evaluation Let’s first start with understanding the metrics available in Retrieval Evaluation Retrieval Evaluation Section: Understanding Metrics in Retrieval Evaluation:: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics: Hit Rate and Mean Reciprocal Rank (MRR). Let’s delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct an...<br>**Metadata:** {'source': '/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"What is the best way to evaluate the performance of the different rerankers in the RAG system?\"\n",
    ")\n",
    "\n",
    "display_response(\n",
    "    response, show_source=True, source_length=500, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** **Hit Rate** is a metric used to evaluate the performance of rerankers in the RAG (Retrieval-Augmented Generation) system. It measures the proportion of queries for which the correct answer or relevant document appears within the top-k retrieved documents. Essentially, it evaluates how often the system successfully identifies the correct information within its top few guesses. \n",
       "\n",
       "For example, if the Hit Rate is calculated for the top-5 documents (Hit@5), it indicates the percentage of queries where the correct answer is found among the first five retrieved documents. A higher Hit Rate means the system is more effective at retrieving relevant information quickly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** e469c097-859b-4b52-94f4-f389436a9764<br>**Similarity:** 0.9029674<br>**Text:** Let’s delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses. Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals...<br>**Metadata:** {'source': '/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** ebb4aab6-352a-45ed-8a07-7fd6a205cdb5<br>**Similarity:** 0.8895761<br>**Text:** Hit Rate: Hit Rate measures the proportion of queries for which the correct chunk/ context appears within the top-k results chunks/ contexts. Put simply, it evaluates how frequently our system correctly identifies the chunk within its top-k chunks. Mean Reciprocal Rank (MRR): MRR assesses a system’s accuracy by taking into account the position of the highest-ranking relevant chunk/ context for each query. It calculates the average of the inverse of these positions across all queries. For inst...<br>**Metadata:** {'source': '/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00', 'title': 'LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG', 'url': 'https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00', 'date': 'Jan 31, 2024'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"Hit Rate? What is that?\"\n",
    ")\n",
    "\n",
    "display_response(\n",
    "    response, show_source=True, source_length=500, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The second metric I mentioned is **Mean Reciprocal Rank (MRR)**. Here's a detailed explanation:\n",
       "\n",
       "**Mean Reciprocal Rank (MRR)** is a metric used to evaluate the effectiveness of a retrieval system by considering the rank position of the first relevant document for each query. It is calculated as follows:\n",
       "\n",
       "1. **Reciprocal Rank**: For each query, determine the rank position of the first relevant document. The reciprocal rank is then calculated as \\( \\frac{1}{\\text{rank}} \\). For example, if the first relevant document is at rank 1, the reciprocal rank is 1. If it is at rank 3, the reciprocal rank is \\( \\frac{1}{3} \\).\n",
       "\n",
       "2. **Mean Reciprocal Rank**: The MRR is the average of the reciprocal ranks across all queries. It is given by the formula:\n",
       "   \\[\n",
       "   \\text{MRR} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\text{rank}_i}\n",
       "   \\]\n",
       "   where \\( N \\) is the total number of queries, and \\( \\text{rank}_i \\) is the rank position of the first relevant document for the \\( i \\)-th query.\n",
       "\n",
       "MRR provides a single score that reflects the system's ability to rank relevant documents highly. A higher MRR indicates better performance, as it means relevant documents are appearing closer to the top of the ranked list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"And the second metric you mentione in the previous response?\"\n",
    ")\n",
    "\n",
    "display_response(\n",
    "    response, show_source=True, source_length=500, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a QueryEngineTool for vector search and summary\n",
    "vector_tool = QueryEngineTool(\n",
    "    index.as_query_engine(),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Useful for searching for information.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool(\n",
    "    index.as_query_engine(response_mode = \"tree_summarize\"),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"summary\",\n",
    "        description=\"Useful for summarizing information.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine1 = RouterQueryEngine.from_defaults(\n",
    "    [vector_tool, summary_tool],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Searching for key features of llama-agents requires finding information, which aligns with the purpose of searching for information..\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Key features of llama-agents include:\n",
       "\n",
       "1. **Distributed Service Oriented Architecture**: Each agent can operate as an independently running microservice, managed by a customizable LLM-powered control plane that routes and distributes tasks.\n",
       "2. **Communication via standardized API interfaces**: Agents interface using a central control plane orchestrator and communicate by passing messages through a message queue.\n",
       "3. **Define agentic and explicit orchestration flows**: Developers can either directly define the sequence of interactions between agents or use an \"agentic orchestrator\" to determine relevant agents for tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 92279433-6f7e-44c1-bb53-805a502c65a0<br>**Similarity:** 0.8826333<br>**Text:** Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. llama-agents Subsection: Key Features of llama-agents: - Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. - Communication vi...<br>**Metadata:** {'source': '/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems', 'title': 'Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems', 'url': 'https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems', 'date': 'Jun 26, 2024'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 881ee9fe-b681-432f-a751-d4fa52fcb94e<br>**Similarity:** 0.874347<br>**Text:** We're excited to announce the alpha release of llama-agents, a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. llama-agents Subsection: Key Features of llama-agents: - D...<br>**Metadata:** {'source': '/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems', 'title': 'Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems', 'url': 'https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems', 'date': 'Jun 26, 2024'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine1.query(\"What are key features of llama-agents?\")\n",
    "display_response(\n",
    "    response, show_source=True, source_length=500, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Searching for information is more relevant to understanding why something is not a hit rate..\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Mean reciprocal rank"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** ff55e25c-2a1d-4819-87c9-c445c6c24270<br>**Similarity:** 0.7942804<br>**Text:** SubSubsection: Evaluating the NetworkRetriever NetworkRetriever To evaluate the efficacy of the NetworkRetriever we make use of our test set in order to compute two traditional retrieval metrics, namely: hit rate and mean reciprocal rank. NetworkRetriever - hit rate: a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set. - mean reciprocal rank: similar to hit rate,...<br>**Metadata:** {'source': '/blog/retrieving-privacy-safe-documents-over-a-network', 'title': 'Retrieving Privacy-Safe Documents Over A Network', 'url': 'https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network', 'date': 'Mar 20, 2024'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 9c84211a-37d8-49f7-9101-06a6f39223e2<br>**Similarity:** 0.794103<br>**Text:** NetworkRetriever - hit rate: a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set. - mean reciprocal rank: similar to hit rate, but now we take into account the position of the first retrieved node that shares the same disease label as the test query. If there is no such retrieved node, then the reciprocal rank of the test is equal to 0. The mean reciprocal rank i...<br>**Metadata:** {'source': '/blog/retrieving-privacy-safe-documents-over-a-network', 'title': 'Retrieving Privacy-Safe Documents Over A Network', 'url': 'https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network', 'date': 'Mar 20, 2024'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine1.query(\"Not hit rate\")\n",
    "display_response(\n",
    "    response, show_source=True, source_length=500, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools(\n",
    "    [vector_tool, summary_tool],\n",
    "    llm=Settings.llm,\n",
    "    node_post_processor=[ranker],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step b9270e1d-65aa-4bb0-a275-9b0ab9946deb. Step input: What is the best way to evaluate the performance of the different rerankers in the RAG system?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: vector_search\n",
      "Action Input: {'input': 'best way to evaluate the performance of different rerankers in the RAG system'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: The best way to evaluate the performance of different rerankers in the RAG system is to use metrics such as Hit Rate and Mean Reciprocal Rank (MRR). These metrics provide insights into how effectively the retrieval system is performing, with Hit Rate indicating the fraction of queries where the correct answer is found within the top-k retrieved documents, and MRR measuring the rank of the first correct answer.\n",
      "\u001b[0m> Running step e41aab81-57d2-4d4a-951d-6cfd4c1a7260. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The best way to evaluate the performance of different rerankers in the RAG system is to use metrics such as Hit Rate and Mean Reciprocal Rank (MRR). Hit Rate indicates the fraction of queries where the correct answer is found within the top-k retrieved documents, while MRR measures the rank of the first correct answer. These metrics provide valuable insights into the effectiveness of the retrieval system.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is the best way to evaluate the performance of the different rerankers in the RAG system?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The best way to evaluate the performance of different rerankers in the RAG system is to use metrics such as Hit Rate and Mean Reciprocal Rank (MRR). Hit Rate indicates the fraction of queries where the correct answer is found within the top-k retrieved documents, while MRR measures the rank of the first correct answer. These metrics provide valuable insights into the effectiveness of the retrieval system."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** bfd5fa0b-49f4-4da9-8adc-4b9151a24081<br>**Similarity:** 0.8654271<br>**Text:** However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on. It’s worth mentioning that these results provide a solid insight into performance for this particular dataset and task. However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on. The table below showcases the evaluation results based on the metrics of Hit Rate a...<br>**Metadata:** {'source': '/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 2d0a3c89-1155-45a4-b2e0-12e0fd338c7d<br>**Similarity:** 0.8604243<br>**Text:** How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we’ll use the Retrieval Evaluation module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Retrieval Evaluation Let’s first start with understanding the metrics available in Retrieval Evaluation Retrieval Evaluation Section: Understanding Metrics in Retrieval Evaluation:: To gauge the efficacy of our retrieval syst...<br>**Metadata:** {'source': '/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response, show_source=True, source_length=500, show_source_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 5e7904fb-1327-48d8-980f-19b85d133825. Step input: Can you summarize your previous answer?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me summarize my previous answer.\n",
      "Action: summary\n",
      "Action Input: {'input': 'The best way to evaluate the performance of different rerankers in the RAG system is to use metrics such as Hit Rate and Mean Reciprocal Rank (MRR). Hit Rate indicates the fraction of queries where the correct answer is found within the top-k retrieved documents, while MRR measures the rank of the first correct answer. These metrics provide valuable insights into the effectiveness of the retrieval system.'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: The best way to evaluate the performance of different rerankers in the RAG system is to use metrics such as Hit Rate and Mean Reciprocal Rank (MRR). Hit Rate indicates the fraction of queries where the correct answer is found within the top-k retrieved documents, while MRR measures the rank of the first correct answer. These metrics provide valuable insights into the effectiveness of the retrieval system.\n",
      "\u001b[0m> Running step b5a9e1ea-1a52-43a3-a935-b80f51a585ca. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The best way to evaluate the performance of different rerankers in the RAG system is to use metrics such as Hit Rate and Mean Reciprocal Rank (MRR). These metrics provide valuable insights into the effectiveness of the retrieval system.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Can you summarize your previous answer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e469c097-859b-4b52-94f4-f389436a9764': {'source': '/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83',\n",
       "  'title': 'Boosting RAG: Picking the Best Embedding & Reranker models',\n",
       "  'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83',\n",
       "  'date': 'Nov 3, 2023'},\n",
       " '4bde7f43-606f-47db-b581-bb338b60efe4': {'source': '/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83',\n",
       "  'title': 'Boosting RAG: Picking the Best Embedding & Reranker models',\n",
       "  'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83',\n",
       "  'date': 'Nov 3, 2023'}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 415d42a2-7389-4394-b2bc-31f1f24b7f4d. Step input: hi\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: Hello! How can I assist you today?\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='Hello! How can I assist you today?', sources=[], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
