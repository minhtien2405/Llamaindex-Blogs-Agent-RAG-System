{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlj8iIOIoalC"
      },
      "source": [
        "<h3 style='color:#1D3E06'><b>TABLE OF CONTENTS</b></h3>\n",
        "\n",
        "* [1. Initialization](#1)\n",
        "* [2. Importing the required libraries](#2)\n",
        "* [3. Data Crawling and Cleaning](#3)\n",
        "  * [3.1 Data Crawling](#3.1)\n",
        "  * [3.2 Data Cleaning](#3.2)\n",
        "* [4. Chunking and Vector Embeddings](#4)\n",
        "  * [4.1 LLM Loading](#4.1)\n",
        "  * [4.2 Data Loading and Chunking](#4.2)\n",
        "  * [4.3 Data Embedding and Vector Store ](#4.3)\n",
        "* [5. Query Engine](#5)\n",
        "  * [5.1 Public Testcases ](#5.1)\n",
        "  * [5.2 Router Query Engine](#5.2)\n",
        "  * [5.3 User Interface Demo](#5.3)\n",
        "* [6. Conclusion](#6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAfb9LXfoalD"
      },
      "source": [
        "<a id=\"1\"></a>\n",
        "## <div style=\"text-align: left; background-color:#DEF5B9; font-family: Trebuchet MS; color:#1D3E06; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green\">1. Initialization  </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwzpCF4_yyEA",
        "outputId": "e17098ff-3b19-4edd-e8ed-4e37b984de82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-3gkR-zyyEA",
        "outputId": "0bf3d13b-1453-4936-f596-79f8c065f244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jul 26 11:11:51 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AioWtNw6_92f",
        "outputId": "388553e7-5bd3-4ded-cf5b-ee1ab66cb0e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.10.58)\n",
            "Requirement already satisfied: llama-index-llms-huggingface in /usr/local/lib/python3.10/dist-packages (0.2.5)\n",
            "Requirement already satisfied: llama-index-vector-stores-qdrant in /usr/local/lib/python3.10/dist-packages (0.2.14)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.9)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.13)\n",
            "Requirement already satisfied: llama-index-core==0.10.58 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.10.58)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.11)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.7)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.27 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.27)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.8)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.7)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.31)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.58->llama-index) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (1.37.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.58->llama-index) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.23.5)\n",
            "Requirement already satisfied: text-generation<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (2.3.1+cu121)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.60.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-qdrant) (1.65.1)\n",
            "Requirement already satisfied: qdrant-client>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-qdrant) (1.10.1)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: minijinja>=1.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.0.1)\n",
            "Requirement already satisfied: llama-cloud>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (0.0.11)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse>=0.1.2->llama-index) (0.4.9)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (1.65.1)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.10.1)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.8.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.58->llama-index) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.58->llama-index) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.58->llama-index) (2024.7.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.11.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.5.82)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.32.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.58->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (5.27.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (71.0.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.58->llama-index) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.58->llama-index) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.58->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core==0.10.58->llama-index) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (4.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.58->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.58->llama-index) (1.4.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.58->llama-index) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.20.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.58->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.58->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core==0.10.58->llama-index) (3.21.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.58->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.58->llama-index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.58->llama-index) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core==0.10.58->llama-index) (1.2.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (4.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.58->llama-index) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-llms-huggingface llama-index-vector-stores-qdrant llama-index-embeddings-huggingface transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "50Z47i5O1nNh",
        "outputId": "2fb73e8b-62b7-4d21-c945-0bda1f9f1423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gZzyXx0ezouK",
        "outputId": "88fa8d0a-d036-4da1-bdd4-34f47dc81f99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flash_attn in /usr/local/lib/python3.10/dist-packages (2.6.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash_attn) (2.3.1+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash_attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash_attn) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash_attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash_attn) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install flash_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4ipKCgM-zqSz",
        "outputId": "2b85da2f-3620-4773-c0e3-ac47ca4346bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.39.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==1.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.1.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.5)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.5)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.1->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.1->gradio) (11.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.4)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.2.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.22.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz41gUgpoalF"
      },
      "source": [
        "<a id=\"2\"></a>\n",
        "## <div style=\"text-align: left; background-color:#DEF5B9; font-family: Trebuchet MS; color:#1D3E06; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green\">2. Importing the required libraries 📚</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Aa9S2eS_yYv",
        "outputId": "d9c1dac7-7b04-4e38-ddc2-218c77689cef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import requests\n",
        "import getpass\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from time import time\n",
        "import gradio as gr\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings, Document, VectorStoreIndex, StorageContext\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.response.notebook_utils import display_response\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.agent import ReActAgent, FunctionCallingAgentWorker, AgentRunner\n",
        "from qdrant_client import QdrantClient\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdArQO42oalG"
      },
      "source": [
        "<a id=\"3\"></a>\n",
        "## <div style=\"text-align: left; background-color:#DEF5B9; font-family: Trebuchet MS; color:#1D3E06; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green\">3. Data Crawling and Cleaning 🛠️</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A10eMYwoalG"
      },
      "source": [
        "<a id=\"3.1\"></a>\n",
        "### <div style=\"text-align: left; background-color:#F0DCED; font-family:Trebuchet MS;color:#8F2A46; padding: 14px; line-height: 1;border-radius:10px;border-style: solid;border-color: dark pink\">3.1 Data Crawling 🍷</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TqPe1En-8wF",
        "outputId": "140ebcda-d2b3-415c-a897-f61daa6c987a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 200\n",
            "Page Title: Blog — LlamaIndex, Data Framework for LLM Applications\n"
          ]
        }
      ],
      "source": [
        "# Set up headers to mimic a browser request\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Making a GET request with headers\n",
        "try:\n",
        "    r = requests.get('https://www.llamaindex.ai/blog', headers=headers, timeout=10)\n",
        "    r.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    # Print status code\n",
        "    print(f\"Status Code: {r.status_code}\")\n",
        "\n",
        "    # Parsing the HTML\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "    # Print the title of the page\n",
        "    print(f\"Page Title: {soup.title.string if soup.title else 'No title found'}\")\n",
        "\n",
        "except requests.RequestException as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8WWWRapQ_Djp",
        "outputId": "5d020d43-21c3-4b50-8842-c9176afd5057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Introducing LlamaExtract Beta: structured data extraction in just a few clicks\n",
            "Date: Jul 25, 2024\n",
            "URL: https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-07-23\n",
            "Date: Jul 23, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23\n",
            "---\n",
            "Title: Improving Vector Search - Reranking with PostgresML and LlamaIndex\n",
            "Date: Jul 19, 2024\n",
            "URL: https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex\n",
            "---\n",
            "Title: The latest updates to LlamaCloud\n",
            "Date: Jul 19, 2024\n",
            "URL: https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud\n",
            "---\n",
            "Title: Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud\n",
            "Date: Jul 17, 2024\n",
            "URL: https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud\n",
            "---\n",
            "Title: Building a multi-agent concierge system\n",
            "Date: Jul 17, 2024\n",
            "URL: https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-07-16\n",
            "Date: Jul 16, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16\n",
            "---\n",
            "Title: Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications\n",
            "Date: Jul 11, 2024\n",
            "URL: https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications\n",
            "---\n",
            "Title: Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex\n",
            "Date: Jul 10, 2024\n",
            "URL: https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-07-09\n",
            "Date: Jul 9, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09\n",
            "---\n",
            "Title: LlamaCloud - Built for Enterprise LLM App Builders\n",
            "Date: Jul 9, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-07-02\n",
            "Date: Jul 2, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02\n",
            "---\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            "Date: Jun 26, 2024\n",
            "URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-06-25\n",
            "Date: Jun 25, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-06-18\n",
            "Date: Jun 18, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-18\n",
            "---\n",
            "Title: Customizing property graph index in LlamaIndex\n",
            "Date: Jun 11, 2024\n",
            "URL: https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-06-11\n",
            "Date: Jun 11, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-06-04\n",
            "Date: Jun 4, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-04\n",
            "---\n",
            "Title: Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs\n",
            "Date: May 29, 2024\n",
            "URL: https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms\n",
            "---\n",
            "Title: Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
            "Date: May 28, 2024\n",
            "URL: https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-05-28\n",
            "Date: May 28, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28\n",
            "---\n",
            "Title: Automate online tasks with MultiOn and LlamaIndex\n",
            "Date: May 23, 2024\n",
            "URL: https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex\n",
            "---\n",
            "Title: Batch inference with MyMagic AI and LlamaIndex\n",
            "Date: May 22, 2024\n",
            "URL: https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-05-21\n",
            "Date: May 21, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-21\n",
            "---\n",
            "Title: Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions\n",
            "Date: May 21, 2024\n",
            "URL: https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions\n",
            "---\n",
            "Title: Using LlamaIndex and llamafile to build a local, private research assistant\n",
            "Date: May 14, 2024\n",
            "URL: https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-05-14\n",
            "Date: May 14, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-14\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-05-07\n",
            "Date: May 7, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-07\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-04-30\n",
            "Date: Apr 30, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-30\n",
            "---\n",
            "Title: Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB\n",
            "Date: Apr 29, 2024\n",
            "URL: https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-04-23\n",
            "Date: Apr 23, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-04-16\n",
            "Date: Apr 16, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-04-09\n",
            "Date: Apr 9, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-04-02\n",
            "Date: Apr 2, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-03-26\n",
            "Date: Mar 26, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-26\n",
            "---\n",
            "Title: Secure RAG with LlamaIndex and LLM Guard by Protect AI\n",
            "Date: Mar 20, 2024\n",
            "URL: https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai\n",
            "---\n",
            "Title: Retrieving Privacy-Safe Documents Over A Network\n",
            "Date: Mar 20, 2024\n",
            "URL: https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network\n",
            "---\n",
            "Title: Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations\n",
            "Date: Mar 19, 2024\n",
            "URL: https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-03-19\n",
            "Date: Mar 19, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19\n",
            "---\n",
            "Title: One-click Open Source RAG Observability with Langfuse\n",
            "Date: Mar 18, 2024\n",
            "URL: https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse\n",
            "---\n",
            "Title: LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
            "Date: Mar 18, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim\n",
            "---\n",
            "Title: PII Detector: hacking privacy in RAG\n",
            "Date: Mar 13, 2024\n",
            "URL: https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag\n",
            "---\n",
            "Title: Launching the first GenAI-native document parsing platform\n",
            "Date: Mar 13, 2024\n",
            "URL: https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-03-12\n",
            "Date: Mar 12, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-12\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024-03-05\n",
            "Date: Mar 5, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05\n",
            "---\n",
            "Title: Towards Long Context RAG\n",
            "Date: Mar 1, 2024\n",
            "URL: https://www.llamaindex.ai/blog/towards-long-context-rag\n",
            "---\n",
            "Title: Unlocking the 3rd Dimension for Generative AI (Part 1)\n",
            "Date: Feb 29, 2024\n",
            "URL: https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1\n",
            "---\n",
            "Title: Querying a network of knowledge with llama-index-networks\n",
            "Date: Feb 27, 2024\n",
            "URL: https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024–02–27\n",
            "Date: Feb 27, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824\n",
            "---\n",
            "Title: Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot\n",
            "Date: Feb 24, 2024\n",
            "URL: https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3\n",
            "---\n",
            "Title: Introducing LlamaCloud and LlamaParse\n",
            "Date: Feb 20, 2024\n",
            "URL: https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024–02–20: introducing LlamaCloud\n",
            "Date: Feb 20, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4\n",
            "---\n",
            "Title: MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB\n",
            "Date: Feb 17, 2024\n",
            "URL: https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2023–02–13\n",
            "Date: Feb 13, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5\n",
            "---\n",
            "Title: Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning\n",
            "Date: Feb 12, 2024\n",
            "URL: https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f\n",
            "---\n",
            "Title: LlamaIndex v0.10\n",
            "Date: Feb 12, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8\n",
            "---\n",
            "Title: How to build LLM Agents in TypeScript with LlamaIndex.TS\n",
            "Date: Feb 8, 2024\n",
            "URL: https://www.llamaindex.ai/blog/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024–02–06\n",
            "Date: Feb 6, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-06-9a303130ad9f\n",
            "---\n",
            "Title: RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex\n",
            "Date: Feb 2, 2024\n",
            "URL: https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089\n",
            "---\n",
            "Title: LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG\n",
            "Date: Jan 31, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00\n",
            "---\n",
            "Title: Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex\n",
            "Date: Jan 30, 2024\n",
            "URL: https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4\n",
            "---\n",
            "Title: Agentic RAG With LlamaIndex\n",
            "Date: Jan 30, 2024\n",
            "URL: https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024–01–30\n",
            "Date: Jan 30, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef\n",
            "---\n",
            "Title: Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex\n",
            "Date: Jan 26, 2024\n",
            "URL: https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9\n",
            "---\n",
            "Title: Introducing the LlamaIndex retrieval-augmented generation command-line tool\n",
            "Date: Jan 26, 2024\n",
            "URL: https://www.llamaindex.ai/blog/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41\n",
            "---\n",
            "Title: Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines\n",
            "Date: Jan 25, 2024\n",
            "URL: https://www.llamaindex.ai/blog/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf\n",
            "---\n",
            "Title: Building a Slack bot that learns with LlamaIndex, Qdrant and Render\n",
            "Date: Jan 25, 2024\n",
            "URL: https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024–01–23\n",
            "Date: Jan 23, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024–01–16\n",
            "Date: Jan 16, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d\n",
            "---\n",
            "Title: Building Multi-Tenancy RAG System with LlamaIndex\n",
            "Date: Jan 15, 2024\n",
            "URL: https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b\n",
            "---\n",
            "Title: AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)\n",
            "Date: Jan 14, 2024\n",
            "URL: https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a\n",
            "---\n",
            "Title: Free Advanced RAG Certification course with Activeloop and LlamaIndex\n",
            "Date: Jan 11, 2024\n",
            "URL: https://www.llamaindex.ai/blog/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024–01–09\n",
            "Date: Jan 9, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6\n",
            "---\n",
            "Title: Introducing Query Pipelines\n",
            "Date: Jan 8, 2024\n",
            "URL: https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537\n",
            "---\n",
            "Title: A Cheat Sheet and Some Recipes For Building Advanced RAG\n",
            "Date: Jan 5, 2024\n",
            "URL: https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b\n",
            "---\n",
            "Title: Building An Intelligent Query-Response System with LlamaIndex and OpenLLM\n",
            "Date: Jan 3, 2024\n",
            "URL: https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf\n",
            "---\n",
            "Title: Scaling LlamaIndex with AWS and Hugging Face\n",
            "Date: Jan 2, 2024\n",
            "URL: https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2024–01–02\n",
            "Date: Jan 2, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842\n",
            "---\n",
            "Title: Running Mixtral 8x7 locally with LlamaIndex and Ollama\n",
            "Date: Dec 21, 2023\n",
            "URL: https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab\n",
            "---\n",
            "Title: Two new llama-datasets and a Gemini vs. GPT showdown\n",
            "Date: Dec 20, 2023\n",
            "URL: https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2023–12–19\n",
            "Date: Dec 19, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726\n",
            "---\n",
            "Title: Multimodal RAG pipeline with LlamaIndex and Neo4j\n",
            "Date: Dec 18, 2023\n",
            "URL: https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206\n",
            "---\n",
            "Title: Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit\n",
            "Date: Dec 17, 2023\n",
            "URL: https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9\n",
            "---\n",
            "Title: LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model\n",
            "Date: Dec 15, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277\n",
            "---\n",
            "Title: How to train a custom GPT on your data with EmbedAI + LlamaIndex\n",
            "Date: Dec 14, 2023\n",
            "URL: https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070\n",
            "---\n",
            "Title: LlamaIndex + Gemini\n",
            "Date: Dec 13, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2023–12–12\n",
            "Date: Dec 12, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e\n",
            "---\n",
            "Title: Bridging the Language Gap in Programming: Introducing AutoTranslateDoc\n",
            "Date: Dec 8, 2023\n",
            "URL: https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8\n",
            "---\n",
            "Title: LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis\n",
            "Date: Dec 6, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2023–12–05\n",
            "Date: Dec 5, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264\n",
            "---\n",
            "Title: Introducing Llama Datasets 🦙📝\n",
            "Date: Dec 4, 2023\n",
            "URL: https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e\n",
            "---\n",
            "Title: OpenAI Cookbook: Evaluating RAG systems\n",
            "Date: Nov 28, 2023\n",
            "URL: https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2023–11–28\n",
            "Date: Nov 28, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786\n",
            "---\n",
            "Title: Multimodal RAG: Building ‘AInimal Go!’, a Pokémon Go-Inspired App with ResNet, Cohere and Llamaindex\n",
            "Date: Nov 27, 2023\n",
            "URL: https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97\n",
            "---\n",
            "Title: Introducing Llama Packs\n",
            "Date: Nov 22, 2023\n",
            "URL: https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a\n",
            "---\n",
            "Title: Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data\n",
            "Date: Nov 21, 2023\n",
            "URL: https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2023–11–21\n",
            "Date: Nov 21, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-21-aa3a71e339f8\n",
            "---\n",
            "Title: Becoming Proficient in Document Extraction\n",
            "Date: Nov 20, 2023\n",
            "URL: https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5\n",
            "---\n",
            "Title: Shipping your Retrieval-Augmented Generation app to production with create-llama\n",
            "Date: Nov 20, 2023\n",
            "URL: https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d\n",
            "---\n",
            "Title: GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.\n",
            "Date: Nov 17, 2023\n",
            "URL: https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9\n",
            "---\n",
            "Title: Evaluating Multi-Modal Retrieval-Augmented Generation\n",
            "Date: Nov 16, 2023\n",
            "URL: https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428\n",
            "---\n",
            "Title: Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex\n",
            "Date: Nov 16, 2023\n",
            "URL: https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b\n",
            "---\n",
            "Title: Announcing LlamaIndex 0.9\n",
            "Date: Nov 15, 2023\n",
            "URL: https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945\n",
            "---\n",
            "Title: create-llama, a command line tool to generate LlamaIndex apps\n",
            "Date: Nov 14, 2023\n",
            "URL: https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2023–11–14\n",
            "Date: Nov 14, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a\n",
            "---\n",
            "Title: LlamaIndex turns 1!\n",
            "Date: Nov 13, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3\n",
            "---\n",
            "Title: Multi-Modal RAG\n",
            "Date: Nov 10, 2023\n",
            "URL: https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea\n",
            "---\n",
            "Title: Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex\n",
            "Date: Nov 8, 2023\n",
            "URL: https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2023-11–07\n",
            "Date: Nov 8, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa\n",
            "---\n",
            "Title: LlamaIndex news special edition: OpenAI developer day!\n",
            "Date: Nov 7, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2\n",
            "---\n",
            "Title: LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression\n",
            "Date: Nov 6, 2023\n",
            "URL: https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7\n",
            "---\n",
            "Title: Boosting RAG: Picking the Best Embedding & Reranker models\n",
            "Date: Nov 3, 2023\n",
            "URL: https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2023–10–31\n",
            "Date: Oct 31, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c\n",
            "---\n",
            "Title: NewsGPT(Neotice): Summarize news articles with LlamaIndex — Hackathon winning app\n",
            "Date: Oct 27, 2023\n",
            "URL: https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11\n",
            "---\n",
            "Title: LlamaIndex newsletter 2023–10–24\n",
            "Date: Oct 24, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3\n",
            "---\n",
            "Title: NVIDIA Research: RAG with Long Context LLMs\n",
            "Date: Oct 22, 2023\n",
            "URL: https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4\n",
            "---\n",
            "Title: Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser\n",
            "Date: Oct 18, 2023\n",
            "URL: https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125\n",
            "---\n",
            "Title: Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)\n",
            "Date: Oct 18, 2023\n",
            "URL: https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d\n",
            "---\n",
            "Title: How I built the Streamlit LLM Hackathon winning app — FinSight using LlamaIndex.\n",
            "Date: Oct 17, 2023\n",
            "URL: https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0\n",
            "---\n",
            "Title: LlamaIndex Newsletter 2023–10–17\n",
            "Date: Oct 17, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2\n",
            "---\n",
            "Title: LlamaIndex update 2023–10–10\n",
            "Date: Oct 10, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9\n",
            "---\n",
            "Title: Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex\n",
            "Date: Oct 5, 2023\n",
            "URL: https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n",
            "---\n",
            "Title: LlamaIndex + Laurie Voss: an alpaca joins the llamas\n",
            "Date: Oct 2, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff\n",
            "---\n",
            "Title: Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications\n",
            "Date: Sep 27, 2023\n",
            "URL: https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0\n",
            "---\n",
            "Title: LlamaIndex Update — 20/09/2023\n",
            "Date: Sep 21, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac\n",
            "---\n",
            "Title: LlamaIndex + Vectara\n",
            "Date: Sep 12, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb\n",
            "---\n",
            "Title: LlamaIndex Update — 09/03/2023\n",
            "Date: Sep 6, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b\n",
            "---\n",
            "Title: Fine-Tuning a Linear Adapter for Any Embedding Model\n",
            "Date: Sep 6, 2023\n",
            "URL: https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383\n",
            "---\n",
            "Title: ChatGPT’s Knowledge is Two Years Old: What to do if you’re building applications?\n",
            "Date: Sep 1, 2023\n",
            "URL: https://www.llamaindex.ai/blog/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c\n",
            "---\n",
            "Title: Introducing Airbyte sources within LlamaIndex\n",
            "Date: Aug 29, 2023\n",
            "URL: https://www.llamaindex.ai/blog/introducing-airbyte-sources-within-llamaindex-42209071722f\n",
            "---\n",
            "Title: LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
            "Date: Aug 29, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af\n",
            "---\n",
            "Title: Fine-Tuning Embeddings for RAG with Synthetic Data\n",
            "Date: Aug 25, 2023\n",
            "URL: https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971\n",
            "---\n",
            "Title: LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs\n",
            "Date: Aug 21, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f\n",
            "---\n",
            "Title: Easily Finetune Llama 2 for Your Text-to-SQL Applications\n",
            "Date: Aug 17, 2023\n",
            "URL: https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d\n",
            "---\n",
            "Title: LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews\n",
            "Date: Aug 12, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b\n",
            "---\n",
            "Title: Zep and LlamaIndex: A Vector Store Walkthrough\n",
            "Date: Aug 11, 2023\n",
            "URL: https://www.llamaindex.ai/blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc\n",
            "---\n",
            "Title: LlamaIndex Update — 08/01/2023\n",
            "Date: Aug 1, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897\n",
            "---\n",
            "Title: Data Agents + Zapier NLA\n",
            "Date: Jul 25, 2023\n",
            "URL: https://www.llamaindex.ai/blog/data-agents-zapier-nla-67146395ce1\n",
            "---\n",
            "Title: Introducing LlamaIndex.TS\n",
            "Date: Jul 24, 2023\n",
            "URL: https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab\n",
            "---\n",
            "Title: Building Better Tools for LLM Agents\n",
            "Date: Jul 17, 2023\n",
            "URL: https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11\n",
            "---\n",
            "Title: Data Agents\n",
            "Date: Jul 12, 2023\n",
            "URL: https://www.llamaindex.ai/blog/data-agents-eed797d7972f\n",
            "---\n",
            "Title: LlamaIndex Update — 07/11/2023\n",
            "Date: Jul 10, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb\n",
            "---\n",
            "Title: LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development\n",
            "Date: Jul 4, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024\n",
            "---\n",
            "Title: Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)\n",
            "Date: Jun 30, 2023\n",
            "URL: https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0\n",
            "---\n",
            "Title: Enriching LlamaIndex Models with GraphQL and Graph Databases\n",
            "Date: Jun 30, 2023\n",
            "URL: https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7\n",
            "---\n",
            "Title: Build and Scale a Powerful Query Engine with LlamaIndex and Ray\n",
            "Date: Jun 27, 2023\n",
            "URL: https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4\n",
            "---\n",
            "Title: LlamaIndex Update — 06/26/2023\n",
            "Date: Jun 26, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84\n",
            "---\n",
            "Title: Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
            "Date: Jun 23, 2023\n",
            "URL: https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c\n",
            "---\n",
            "Title: Llama Index & Prem AI Join Forces\n",
            "Date: Jun 23, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec\n",
            "---\n",
            "Title: LlamaIndex and Weaviate\n",
            "Date: Jun 22, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4\n",
            "---\n",
            "Title: LlamaIndex and Transformers Agents\n",
            "Date: Jun 8, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6\n",
            "---\n",
            "Title: Building the data framework for LLMs\n",
            "Date: Jun 6, 2023\n",
            "URL: https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e\n",
            "---\n",
            "Title: Vellum <> LlamaIndex Integration\n",
            "Date: Jun 5, 2023\n",
            "URL: https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f\n",
            "---\n",
            "Title: Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation\n",
            "Date: May 28, 2023\n",
            "URL: https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b\n",
            "---\n",
            "Title: Dumber LLM Agents Need More Constraints and Better Tools\n",
            "Date: May 23, 2023\n",
            "URL: https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12\n",
            "---\n",
            "Title: Build a ChatGPT with your Private Data using LlamaIndex and MongoDB\n",
            "Date: May 18, 2023\n",
            "URL: https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c\n",
            "---\n",
            "Title: Using LLM’s for Retrieval and Reranking\n",
            "Date: May 17, 2023\n",
            "URL: https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6\n",
            "---\n",
            "Title: Testing Anthropic Claude’s 100k-token window on SEC 10-K Filings\n",
            "Date: May 12, 2023\n",
            "URL: https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba\n",
            "---\n",
            "Title: LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)\n",
            "Date: May 10, 2023\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595\n",
            "---\n",
            "Title: A New Document Summary Index for LLM-powered QA Systems\n",
            "Date: May 8, 2023\n",
            "URL: https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec\n",
            "---\n",
            "Title: Building and Evaluating a QA System with LlamaIndex\n",
            "Date: May 7, 2023\n",
            "URL: https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Find all blog post cards\n",
        "blog_cards = soup.find_all('div', class_='CardBlog_card__mm0Zw')\n",
        "base_url = \"https://www.llamaindex.ai\"\n",
        "\n",
        "# Extract and print the main text from each card\n",
        "for card in blog_cards:\n",
        "    # Extract title\n",
        "    title_element = card.find('p', class_='CardBlog_title__qC51U').find('a')\n",
        "    title = title_element.text.strip()\n",
        "    url = base_url + title_element['href']\n",
        "\n",
        "    # Extract publication date\n",
        "    date = card.find('p', class_='Text_text__zPO0D Text_text-size-16__PkjFu').text.strip()\n",
        "\n",
        "    # Print the extracted information\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"Date: {date}\")\n",
        "    print(f\"URL: {url}\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JdXaSO3e_ond"
      },
      "outputs": [],
      "source": [
        "with open('llama_blog.txt', 'w') as f:\n",
        "    for card in blog_cards:\n",
        "        title_element = card.find('p', class_='CardBlog_title__qC51U').find('a')\n",
        "        title = title_element.text.strip()\n",
        "        url = base_url + title_element['href']\n",
        "        date = card.find('p', class_='Text_text__zPO0D Text_text-size-16__PkjFu').text.strip()\n",
        "        f.write(f\"Title: {title}\\n\")\n",
        "        f.write(f\"Date: {date}\\n\")\n",
        "        f.write(f\"URL: {url}\\n\")\n",
        "        f.write(\"---\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u7uuaQ2b_sA1"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=['source', 'title', 'url', 'date', 'content'])\n",
        "\n",
        "for card in blog_cards:\n",
        "    title_element = card.find('p', class_='CardBlog_title__qC51U').find('a')\n",
        "    title = title_element.text.strip()\n",
        "    source = title_element['href']\n",
        "    url = base_url + title_element['href']\n",
        "    date = card.find('p', class_='Text_text__zPO0D Text_text-size-16__PkjFu').text.strip()\n",
        "\n",
        "    df.loc[len(df)] = [source, title, url, date, '']\n",
        "\n",
        "df.to_csv('llama_blog.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q1KcuPJmanSA",
        "outputId": "760909d9-1f7f-4ca8-c65b-02acaa4f5cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We're excited to announce the alpha release of llama-agents, a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life.\n",
            "\n",
            "llama-agents\n",
            "\n",
            "\n",
            "Section: Key Features of llama-agents:\n",
            "- Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
            "- Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
            "- Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.\n",
            "- Ease of deployment: launch, scale and monitor each agent and your control plane independently.\n",
            "- Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service\n",
            "Let's dive into how you can start using llama-agents to build your own multi-agent systems.\n",
            "\n",
            "\n",
            "Section: Getting Started with llama-agents:\n",
            "First, install the framework using pip:\n",
            "\n",
            "pip install llama-agents llama-index-agent-openai\n",
            "\n",
            "\n",
            "Subsection: Basic System Setup:\n",
            "Here's a simple example of how to set up a basic multi-agent system using llama-agents. First we’ll bring in our dependencies and set up our control plane, which contains our LLM-powered orchestrator\n",
            "\n",
            "import dotenv\n",
            "dotenv.load_dotenv() # our .env file defines OPENAI_API_KEY\n",
            "from llama_agents import (\n",
            "    AgentService,\n",
            "    ControlPlaneServer,\n",
            "    SimpleMessageQueue,\n",
            "    AgentOrchestrator,\n",
            ")\n",
            "from llama_index.core.agent import FunctionCallingAgentWorker\n",
            "from llama_index.core.tools import FunctionTool\n",
            "from llama_index.llms.openai import OpenAI\n",
            "import logging\n",
            "\n",
            "# turn on logging so we can see the system working\n",
            "logging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n",
            "\n",
            "# Set up the message queue and control plane\n",
            "message_queue = SimpleMessageQueue()\n",
            "control_plane = ControlPlaneServer(\n",
            "    message_queue=message_queue,\n",
            "    orchestrator=AgentOrchestrator(llm=OpenAI()),\n",
            ")\n",
            "Next we create our tools using LlamaIndex’s existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice:\n",
            "\n",
            "# create a tool\n",
            "def get_the_secret_fact() -> str:\n",
            "    \"\"\"Returns the secret fact.\"\"\"\n",
            "    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n",
            "\n",
            "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
            "\n",
            "# Define an agent\n",
            "worker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\n",
            "agent = worker.as_agent()\n",
            "\n",
            "# Create an agent service\n",
            "agent_service = AgentService(\n",
            "    agent=agent,\n",
            "    message_queue=message_queue,\n",
            "    description=\"General purpose assistant\",\n",
            "    service_name=\"assistant\",\n",
            ")\n",
            "Finally we launch the service and the control plane. Note that here we’re using a helper function to run a single query through the system and then exit; next we’ll show how to deploy this to production.\n",
            "\n",
            "# Set up the launcher for local testing\n",
            "from llama_agents import LocalLauncher\n",
            "\n",
            "launcher = LocalLauncher(\n",
            "    [agent_service],\n",
            "    control_plane,\n",
            "    message_queue,\n",
            ")\n",
            "\n",
            "# Run a single query through the system\n",
            "result = launcher.launch_single(\"What's the secret fact?\")\n",
            "print(result)\n",
            "\n",
            "\n",
            "Section: Deploying Your Multi-Agent System:\n",
            "Once you've tested your system locally, you can deploy it as a set of services for real production use. Here's how you might set that up. This is similar to the previous example, but we’ve added a second agent service and we’re using a different launcher. Let’s bring in our dependencies and set up our control plane again:\n",
            "\n",
            "import dotenv\n",
            "dotenv.load_dotenv()\n",
            "from llama_agents import (\n",
            "    AgentService,\n",
            "    AgentOrchestrator,\n",
            "    ControlPlaneServer,\n",
            "    SimpleMessageQueue,\n",
            ")\n",
            "\n",
            "from llama_index.core.agent import FunctionCallingAgentWorker\n",
            "from llama_index.core.tools import FunctionTool\n",
            "from llama_index.llms.openai import OpenAI\n",
            "import logging\n",
            "\n",
            "# change logging level to enable or disable more verbose logging\n",
            "logging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n",
            "\n",
            "# create our multi-agent framework components\n",
            "message_queue = SimpleMessageQueue()\n",
            "control_plane = ControlPlaneServer(\n",
            "    message_queue=message_queue,\n",
            "    orchestrator=AgentOrchestrator(llm=OpenAI()),\n",
            ")\n",
            "Then as before we create a tool and an agent, though this time we’ll add a second agent:\n",
            "\n",
            "# create a tool\n",
            "def get_the_secret_fact() -> str:\n",
            "    \"\"\"Returns the secret fact.\"\"\"\n",
            "    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n",
            "\n",
            "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
            "\n",
            "# create our agents\n",
            "worker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\n",
            "worker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\n",
            "agent1 = worker1.as_agent()\n",
            "agent2 = worker2.as_agent()\n",
            "We turn those agents into services:\n",
            "\n",
            "agent_server_1 = AgentService(\n",
            "    agent=agent1,\n",
            "    message_queue=message_queue,\n",
            "    description=\"Useful for getting the secret fact.\",\n",
            "    service_name=\"secret_fact_agent\",\n",
            "    host=\"localhost\",\n",
            "    port=8003\n",
            ")\n",
            "agent_server_2 = AgentService(\n",
            "    agent=agent2,\n",
            "    message_queue=message_queue,\n",
            "    description=\"Useful for getting random dumb facts.\",\n",
            "    service_name=\"dumb_fact_agent\",\n",
            "    host=\"localhost\",\n",
            "    port=8004\n",
            ")\n",
            "And finally we launch each service as an independent agent. Here we’re doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently:\n",
            "\n",
            "from llama_agents import ServerLauncher, CallableMessageConsumer\n",
            "\n",
            "# Additional human consumer\n",
            "def handle_result(message) -> None:\n",
            "    print(f\"Got result:\", message.data)\n",
            "\n",
            "\n",
            "# the final result is published to a \"human\" consumer\n",
            "# so we define one to handle it!\n",
            "human_consumer = CallableMessageConsumer(\n",
            "    handler=handle_result, message_type=\"human\"\n",
            ")\n",
            "\n",
            "# Define Launcher\n",
            "launcher = ServerLauncher(\n",
            "    [agent_server_1, agent_server_2],\n",
            "    control_plane,\n",
            "    message_queue,\n",
            "    additional_consumers=[human_consumer]\n",
            ")\n",
            "\n",
            "launcher.launch_servers()\n",
            "\n",
            "\n",
            "Section: Real-time monitoring:\n",
            "One of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. You launch it like this:\n",
            "\n",
            "llama-agents monitor --control-plane-url http://127.0.0.1:8000\n",
            "Once launched, you get an intuitive, point-and-click terminal application. You can see both of the agents running, and at the bottom you can inject a task like the query “What is the secret fact?” You’ll get a job ID which you can then click on to see your results:\n",
            "\n",
            "\n",
            "Section: Building a Query Rewriting RAG System:\n",
            "Let's look at a more complex example: a Query Rewriting RAG system. This system will rewrite user queries to improve retrieval, then use the rewritten query to perform RAG over a document.\n",
            "This example demonstrates how to create a more sophisticated system that combines query rewriting with RAG to improve question-answering capabilities. See this notebook for a fuller explanation of what’s going on.\n",
            "\n",
            "import dotenv\n",
            "dotenv.load_dotenv() # our .env defines OPENAI_API_KEY\n",
            "from llama_index.core import VectorStoreIndex, Document\n",
            "from llama_index.core.agent import FnAgentWorker\n",
            "from llama_index.core import PromptTemplate\n",
            "from llama_index.core.query_pipeline import QueryPipeline\n",
            "from llama_index.core.query_engine import RetrieverQueryEngine\n",
            "from llama_agents import (\n",
            "    AgentService,\n",
            "    ControlPlaneServer,\n",
            "    SimpleMessageQueue,\n",
            "    PipelineOrchestrator,\n",
            "    ServiceComponent,\n",
            ")\n",
            "from llama_agents.launchers import LocalLauncher\n",
            "from llama_index.llms.openai import OpenAI\n",
            "import logging\n",
            "\n",
            "# change logging level to enable or disable more verbose logging\n",
            "logging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n",
            "\n",
            "# Load and index your document\n",
            "docs = [Document(text=\"The rabbit is a small mammal with long ears and a fluffy tail. His name is Peter.\")]\n",
            "index = VectorStoreIndex.from_documents(docs)\n",
            "\n",
            "# Define a query rewrite agent\n",
            "HYDE_PROMPT_STR = (\n",
            "    \"Please rewrite the following query to include more detail:\\n{query_str}\\n\"\n",
            ")\n",
            "HYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR)\n",
            "\n",
            "def run_hyde_fn(state):\n",
            "    prompt_tmpl, llm, input_str = (\n",
            "        state[\"prompt_tmpl\"],\n",
            "        state[\"llm\"],\n",
            "        state[\"__task__\"].input,\n",
            "    )\n",
            "    qp = QueryPipeline(chain=[prompt_tmpl, llm])\n",
            "    output = qp.run(query_str=input_str)\n",
            "    state[\"__output__\"] = str(output)\n",
            "    return state, True\n",
            "\n",
            "hyde_agent = FnAgentWorker(\n",
            "    fn=run_hyde_fn,\n",
            "    initial_state={\"prompt_tmpl\": HYDE_PROMPT_TMPL, \"llm\": OpenAI()}\n",
            ").as_agent()\n",
            "\n",
            "# Define a RAG agent\n",
            "def run_rag_fn(state):\n",
            "    retriever, llm, input_str = (\n",
            "        state[\"retriever\"],\n",
            "        state[\"llm\"],\n",
            "        state[\"__task__\"].input,\n",
            "    )\n",
            "    query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)\n",
            "    response = query_engine.query(input_str)\n",
            "    state[\"__output__\"] = str(response)\n",
            "    return state, True\n",
            "\n",
            "rag_agent = FnAgentWorker(\n",
            "    fn=run_rag_fn,\n",
            "    initial_state={\"retriever\": index.as_retriever(), \"llm\": OpenAI()}\n",
            ").as_agent()\n",
            "\n",
            "# Set up the multi-agent system\n",
            "message_queue = SimpleMessageQueue()\n",
            "\n",
            "query_rewrite_service = AgentService(\n",
            "    agent=hyde_agent,\n",
            "    message_queue=message_queue,\n",
            "    description=\"Query rewriting service\",\n",
            "    service_name=\"query_rewrite\",\n",
            ")\n",
            "\n",
            "rag_service = AgentService(\n",
            "    agent=rag_agent,\n",
            "    message_queue=message_queue,\n",
            "    description=\"RAG service\",\n",
            "    service_name=\"rag\",\n",
            ")\n",
            "\n",
            "# Create the pipeline\n",
            "pipeline = QueryPipeline(chain=[\n",
            "    ServiceComponent.from_service_definition(query_rewrite_service),\n",
            "    ServiceComponent.from_service_definition(rag_service),\n",
            "])\n",
            "orchestrator = PipelineOrchestrator(pipeline)\n",
            "\n",
            "control_plane = ControlPlaneServer(\n",
            "    message_queue=message_queue,\n",
            "    orchestrator=orchestrator,\n",
            ")\n",
            "\n",
            "# Set up the launcher\n",
            "launcher = LocalLauncher(\n",
            "    [query_rewrite_service, rag_service],\n",
            "    control_plane,\n",
            "    message_queue,\n",
            ")\n",
            "\n",
            "# Run a query\n",
            "result = launcher.launch_single(\"Tell me about rabbits\")\n",
            "print(result)\n",
            "\n",
            "\n",
            "Section: Public roadmap:\n",
            "This is an alpha release, meaning that we’d love your feedback on features to better help you build multi-agent systems in production! We’ve created a public roadmap showing where we plan to go from here. We’re actively seeking public feedback on what works for you and what doesn’t.\n",
            "\n",
            "\n",
            "Section: Dive in!:\n",
            "llama-agents provides a powerful, flexible framework for building complex multi-agent AI systems. Whether you're prototyping a new idea or scaling to production, llama-agents offers the tools you need to bring your AI vision to life. Check out the repo to learn more, especially our library of examples.\n",
            "\n",
            "llama-agents\n",
            "\n",
            "llama-agents\n",
            "We're excited to see what the community builds with llama-agents. Happy coding!\n",
            "\n",
            "llama-agents\n"
          ]
        }
      ],
      "source": [
        "def crawl_website(url):\n",
        "    # Set up headers to mimic a browser request\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        content = soup.find('div', class_='BlogPost_htmlPost__Z5oDL')\n",
        "        all_tags = content.find_all()\n",
        "\n",
        "        tags_to_extract = ['h1', 'h2', 'h3', 'p', 'li', 'code', 'blockquote', 'em']\n",
        "\n",
        "        for tag in all_tags:\n",
        "          if tag.name in tags_to_extract:\n",
        "            if tag.name == 'h1':\n",
        "              print(f\"Title: {tag.text.strip()}\")\n",
        "            elif tag.name == 'h2':\n",
        "              print(f\"\\n\\nSection: {tag.text.strip()}:\")\n",
        "            elif tag.name == 'h3':\n",
        "              print(f\"\\n\\nSubsection: {tag.text.strip()}:\")\n",
        "            elif tag.name == 'li':\n",
        "              print(f\"- {tag.text.strip()}\")\n",
        "            elif tag.name == 'code':\n",
        "              print(f\"\\n{tag.text.strip()}\")\n",
        "            else:\n",
        "              print(tag.text.strip())\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the page. Status code: {reponse.status_code}\")\n",
        "\n",
        "url = \"https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\"\n",
        "crawl_website(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wS2YYpeu_uTe"
      },
      "outputs": [],
      "source": [
        "# Extract the content of the first blog post\n",
        "first_post_url = \"https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba\"\n",
        "\n",
        "try:\n",
        "    r = requests.get(first_post_url, headers=headers, timeout=10)\n",
        "    r.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    content = soup.find('div', class_='BlogPost_htmlPost__Z5oDL').text.strip()\n",
        "\n",
        "\n",
        "    with open('temp.txt', 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "\n",
        "except requests.RequestException as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw982qqe_4jP",
        "outputId": "43a6dae7-e59a-4b06-e01c-7f306c0b1962"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 161/161 [00:52<00:00,  3.09it/s]\n"
          ]
        }
      ],
      "source": [
        "# Extract the content of all blog posts\n",
        "for index in tqdm(df.index):\n",
        "    url = df['url'][index]\n",
        "    try:\n",
        "        r = requests.get(url, headers=headers, timeout=10)\n",
        "        r.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Parsing the HTML\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        content = soup.find('div', class_='BlogPost_htmlPost__Z5oDL')\n",
        "        all_tags = content.find_all()\n",
        "\n",
        "        tags_to_extract = ['h1', 'h2', 'h3', 'p', 'li', 'code', 'blockquote', 'em']\n",
        "        text = str(\"\")\n",
        "\n",
        "        # Extract the content from the tags\n",
        "        for tag in all_tags:\n",
        "          if tag.name in tags_to_extract:\n",
        "            if tag.name == 'h1':\n",
        "              text = text + f\"\\n\\nSection: {tag.text.strip()}:\\n\"\n",
        "            elif tag.name == 'h2':\n",
        "              text = text + f\"\\n\\nSubsection: {tag.text.strip()}:\\n\"\n",
        "            elif tag.name == 'h3':\n",
        "              text = text + f\"\\n\\nSubSubsection: {tag.text.strip():}\\n\"\n",
        "            elif tag.name == 'li':\n",
        "              text = text + f\"- {tag.text.strip()}\\n\"\n",
        "            elif tag.name == 'code':\n",
        "              text = text + f\"\\n{tag.text.strip()}\\n\"\n",
        "            else:\n",
        "              text = text + tag.text.strip() + \"\\n\"\n",
        "\n",
        "        # Update the DataFrame with the extracted content\n",
        "        df.at[index, 'content'] = text\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('llama_blog.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muD2Wk8poalI"
      },
      "source": [
        "<a id=\"3.2\"></a>\n",
        "### <div style=\"text-align: left; background-color:#F0DCED; font-family:Trebuchet MS;color:#8F2A46; padding: 14px; line-height: 1;border-radius:10px;border-style: solid;border-color: dark pink\">3.2 Data Cleaning 🧹</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FpDWYoHEoalI",
        "outputId": "13a0a280-9402-4a94-efd0-ed08bf5d6ebe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "0",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-2348c55a-c345-4cf0-9f7c-e34446cf0a64\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>date</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>/blog/using-llms-for-retrieval-and-reranking-2...</td>\n",
              "      <td>Using LLM’s for Retrieval and Reranking</td>\n",
              "      <td>https://www.llamaindex.ai/blog/using-llms-for-...</td>\n",
              "      <td>May 17, 2023</td>\n",
              "      <td>\\n\\nSection: Summary:\\nThis blog post outlines...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>/blog/testing-anthropic-claudes-100k-token-win...</td>\n",
              "      <td>Testing Anthropic Claude’s 100k-token window o...</td>\n",
              "      <td>https://www.llamaindex.ai/blog/testing-anthrop...</td>\n",
              "      <td>May 12, 2023</td>\n",
              "      <td>Anthropic’s 100K Context Window expansion, jus...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>/blog/llamaindex-on-twiml-ai-a-distilled-summa...</td>\n",
              "      <td>LlamaIndex on TWIML AI: A Distilled Summary (u...</td>\n",
              "      <td>https://www.llamaindex.ai/blog/llamaindex-on-t...</td>\n",
              "      <td>May 10, 2023</td>\n",
              "      <td>\\n\\nSection: Overview:\\nI had the pleasure of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>/blog/a-new-document-summary-index-for-llm-pow...</td>\n",
              "      <td>A New Document Summary Index for LLM-powered Q...</td>\n",
              "      <td>https://www.llamaindex.ai/blog/a-new-document-...</td>\n",
              "      <td>May 8, 2023</td>\n",
              "      <td>In this blog post, we introduce a brand new Ll...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>/blog/building-and-evaluating-a-qa-system-with...</td>\n",
              "      <td>Building and Evaluating a QA System with Llama...</td>\n",
              "      <td>https://www.llamaindex.ai/blog/building-and-ev...</td>\n",
              "      <td>May 7, 2023</td>\n",
              "      <td>\\n\\nSection: Introduction:\\nLlamaIndex (GPT In...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2348c55a-c345-4cf0-9f7c-e34446cf0a64')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2348c55a-c345-4cf0-9f7c-e34446cf0a64 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2348c55a-c345-4cf0-9f7c-e34446cf0a64');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-270c470c-4ca2-482a-afe6-e01814325e39\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-270c470c-4ca2-482a-afe6-e01814325e39')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-270c470c-4ca2-482a-afe6-e01814325e39 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                source  \\\n",
              "156  /blog/using-llms-for-retrieval-and-reranking-2...   \n",
              "157  /blog/testing-anthropic-claudes-100k-token-win...   \n",
              "158  /blog/llamaindex-on-twiml-ai-a-distilled-summa...   \n",
              "159  /blog/a-new-document-summary-index-for-llm-pow...   \n",
              "160  /blog/building-and-evaluating-a-qa-system-with...   \n",
              "\n",
              "                                                 title  \\\n",
              "156            Using LLM’s for Retrieval and Reranking   \n",
              "157  Testing Anthropic Claude’s 100k-token window o...   \n",
              "158  LlamaIndex on TWIML AI: A Distilled Summary (u...   \n",
              "159  A New Document Summary Index for LLM-powered Q...   \n",
              "160  Building and Evaluating a QA System with Llama...   \n",
              "\n",
              "                                                   url          date  \\\n",
              "156  https://www.llamaindex.ai/blog/using-llms-for-...  May 17, 2023   \n",
              "157  https://www.llamaindex.ai/blog/testing-anthrop...  May 12, 2023   \n",
              "158  https://www.llamaindex.ai/blog/llamaindex-on-t...  May 10, 2023   \n",
              "159  https://www.llamaindex.ai/blog/a-new-document-...   May 8, 2023   \n",
              "160  https://www.llamaindex.ai/blog/building-and-ev...   May 7, 2023   \n",
              "\n",
              "                                               content  \n",
              "156  \\n\\nSection: Summary:\\nThis blog post outlines...  \n",
              "157  Anthropic’s 100K Context Window expansion, jus...  \n",
              "158  \\n\\nSection: Overview:\\nI had the pleasure of ...  \n",
              "159  In this blog post, we introduce a brand new Ll...  \n",
              "160  \\n\\nSection: Introduction:\\nLlamaIndex (GPT In...  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('llama_blog.csv')\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZIC-RvbzoalI"
      },
      "outputs": [],
      "source": [
        "# Clean the text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df['title'] = df['title'].apply(clean_text)\n",
        "df['content'] = df['content'].apply(clean_text)\n",
        "\n",
        "df.to_csv('llama_blog_clean.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "po22JTaGoalI",
        "outputId": "b0b6cc32-f343-4cc1-b093-76d697e0e31c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 161,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 161,\n        \"samples\": [\n          \"/blog/llamaindex-turns-1-f69dcdd45fe3\",\n          \"/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa\",\n          \"/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 161,\n        \"samples\": [\n          \"LlamaIndex turns 1!\",\n          \"LlamaIndex Newsletter 2023-11\\u201307\",\n          \"LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 161,\n        \"samples\": [\n          \"https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3\",\n          \"https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa\",\n          \"https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 130,\n        \"samples\": [\n          \"Jan 8, 2024\",\n          \"Feb 13, 2024\",\n          \"May 14, 2024\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 161,\n        \"samples\": [\n          \"It\\u2019s our birthday! One year ago, Jerry pushed his first commit to GPT Index, the project that would become LlamaIndex. It worked with GPT-3, the state of the art model available at the time. That initial version was very simple, but the problem statement \\u2014 and the solution \\u2014 remain the same: one fundamental limitation of GPT-3 is the context size [\\u2026] the ability to feed \\u201cknowledge\\u201d to GPT-3 is mostly limited to this limited prompt size [\\u2026] But what if GPT-3 can have access to potentially a much larger database of knowledge[\\u2026]? one fundamental limitation of GPT-3 is the context size [\\u2026] the ability to feed \\u201cknowledge\\u201d to GPT-3 is mostly limited to this limited prompt size [\\u2026] But what if GPT-3 can have access to potentially a much larger database of knowledge[\\u2026]? one fundamental limitation of GPT-3 is the context size [\\u2026] the ability to feed \\u201cknowledge\\u201d to GPT-3 is mostly limited to this limited prompt size [\\u2026] But what if GPT-3 can have access to potentially a much larger database of knowledge[\\u2026]? Twelve months have passed and there\\u2019s been a tsunami of new developments in the world of generative AI and LLMs, but the reason LlamaIndex was invented remains: even the most sophisticated model isn\\u2019t trained on your data, which can be locked behind an API or in a SQL database, and even the latest GPT-4-Turbo context size of 128,000 tokens isn\\u2019t enough to hold even a relatively modest dataset. Retrieval-Augmented Generation (RAG) is here to stay. Section: Big numbers: At just 1 year old, LlamaIndex has gotten very big. How big? Here\\u2019s some numbers: - Over 450 contributors to our open-source library! - Nearly 3,000 open-source projects depend on LlamaIndex! - Nearly 4,000 members in our Discord (come join us!) - 47,000 lines of Python in the library! (Don\\u2019t worry, it\\u2019s still just 0.5MB to download) - Nearly 900,000 downloads every month! - RAG deployed among popular open-source projects, as well as in production in enterprise settings. Section: Big thanks: But big numbers aside, the thing we\\u2019re proudest of is our community: we have users in (nearly) every country in the world, from single hobby developers to Fortune 500 companies and everyone in between. LlamaIndex\\u2019s founder, Jerry Liu, says: Our community is everything at LlamaIndex. We love seeing the amazing things people are building every day! It\\u2019s what gets us up in the morning and keeps us motivated to keep pushing the boundaries of what developers can do with GenAI. And we\\u2019re especially grateful to the developers who give back by pushing PRs, issues and bug reports. They\\u2019re what makes the open source world go round. Our community is everything at LlamaIndex. We love seeing the amazing things people are building every day! It\\u2019s what gets us up in the morning and keeps us motivated to keep pushing the boundaries of what developers can do with GenAI. And we\\u2019re especially grateful to the developers who give back by pushing PRs, issues and bug reports. They\\u2019re what makes the open source world go round. Our community is everything at LlamaIndex. We love seeing the amazing things people are building every day! It\\u2019s what gets us up in the morning and keeps us motivated to keep pushing the boundaries of what developers can do with GenAI. And we\\u2019re especially grateful to the developers who give back by pushing PRs, issues and bug reports. They\\u2019re what makes the open source world go round. Section: Big milestones: What\\u2019s happened in a year? Well, everything! But here\\u2019s some highlights: - November 2022: Launched GPT Tree Index, a way of organizing information into a tree. Based on the initial interest/traction, we expanded this into a List Index and Keyword Index. Then ChatGPT launched in November - December 2022: Some big feature releases: support for indexing embeddings + vector stores, and initial data loaders for Notion, Slack, and Google Drive - January 2023: LlamaIndex hits Github trending for the first time! - February 2023: We launched LlamaHub with Jesse Zhang, containing an initial repository of data loaders for users to access. We ran a sweepstakes with OctoML and got 50+ data loader submissions! - March 2023: ChatGPT API launched and then Plugins. We scrambled to support the new API + Plugin integrations. - April 2023: We incorporated! - May 2023: At the end of April, we launched 0.6.0, where we completely rewrote the entire framework from the ground-up for greater modularity and composability for different levels of abstraction. - June 2023: We announced that we raised $8.5M in funding! - July 2023: We launched Data Agents + Agent Tools on LlamaHub. We also launched a Typescript package - August 2023: We integrated with OpenAI fine-tuning and launched a variety of LLM and embedding fine-tuning abstractions. - September 2023: We launched secinsights.ai \\u2014 a production-ready full-stack application - October 2023: We launched LlamaIndex Chat \\u2014 a full-stack Typescript template. - November 2023: Went fully multi-modal with the release of GPT-4-vision! Section: Big plans: With all that growth and all those features, what\\u2019s next for us? Stay tuned!\",\n          \"Hi again Llama Fans! \\ud83e\\udd99 We hope you enjoyed our OpenAI Dev Day special edition yesterday! Here\\u2019s our wrap-up of everything else that happened last week. As always, if you\\u2019ve got a project, article, or video that\\u2019s turning heads? We\\u2019re all ears! Drop us a line at news@llamaindex.ai. And for all this goodness delivered directly to you, don\\u2019t forget to subscribe to our newsletter via our website. \\ud83e\\udd29 First, the highlights: - LlamaIndex Chat: We unveiled a customizable LLM chatbot template with system prompts and avatars, all within an open-source MIT-licensed framework using LlamaIndex for TypeScript. Explore the Demo or check the Tweet. - Evaluator Fine-Tuning: We launched a method to enhance LLM output assessment by distilling GPT-4 into GPT-3.5, optimizing both cost and speed. See our Tweet. - ParamTuner: We introduced a new hyperparameter tuning abstraction to refine RAG pipeline performance, featuring objective functions, grid search, and Ray Tune integration. Check out the Notebook and Tweet. - CohereAI Embed v3 & Voyage AI Integration: We strengthened the LlamaIndex RAG pipeline with two powerful embedding model additions: the latest Embed v3 from CohereAI and the high-performing embedding model from Voyage AI. Tweet and tweet. \\u2728 Feature Releases and Enhancements: - We introduced LlamaIndex Chat, a new feature allowing users to create and share custom LLM chatbots tailored to their data, complete with personalized system prompts and avatars. Additionally, we\\u2019re proud to share that it\\u2019s a fully open-source template under the MIT license, crafted using LlamaIndexTS for a seamless start to LLM application development. Demo, Tweet. - We introduced a method for fine-tuning an Evaluator to distill GPT-4 into GPT-3.5, enhancing LLM output assessment while reducing costs and improving speed. Tweet. - We introduced ParamTuner, a hyperparameter tuning abstraction for LlamaIndex RAG, streamlining the process with objective functions and support for grid search, including integration with Ray Tune for enhanced optimization. Notebook, Tweet. ParamTuner \\ud83c\\udfa5 Demos: - GPTDiscord is a versatile LLM-powered Discord bot with over 20 features, including multi-modal image understanding and advanced data analysis. It boasts an infinite conversational memory and the ability to interact with various file types and internet services. Tweet. \\ud83d\\uddfa\\ufe0f Guides: - We shared a guide for integrating Activeloop\\u2019s Deep Memory with LlamaIndex, a module that enhances your embeddings at ingestion and can improve RAG metrics by 15%, all while seamlessly fitting into LlamaIndex\\u2019s automated dataset and vector store features. - We shared a guide inspired by Chengrun Yang and GoogleDeepMind\\u2019s Optimization by Prompting paper, demonstrating how to automate prompt tuning in LlamaIndex RAG pipelines using meta-prompting, boosting evaluation performance while acknowledging the experimental nature of this technique. Optimization by Prompting - We shared a guide on how to implement Emotion Prompting in LlamaIndex, allowing you to enhance your RAG pipeline with various emotional stimuli and evaluate their impact on task performance. - We showcased MongoDB starter kit, a comprehensive LlamaIndex RAG setup with Flask backend, Next frontend, and easy deployment to Render. \\u270d\\ufe0f Tutorials: - Wenqi Glantz made a blog post on deploying the HuggingFace text-embeddings-inference server on an AWS EC2 GPU instance, enhancing LlamaIndex RAG pipeline's performance and results. text-embeddings-inference - Sophia Yang\\u2019s tutorial on Zephyr-7b-beta showcases its leading capabilities in LLM technology, including how it\\u2019s benchmarked with LlamaIndex for diverse AI tasks. - Sudarshan Koirala gave a tutorial on how to build a multi-modal retrieval system with LlamaIndex, Qdrant, and bge/CLIP embeddings. - Sophia Yang\\u2019s gave another tutorial, this time on Small-to-Big Retrieval with LlamaIndex in building advanced RAG systems. - Ravi Theja\\u2019s tutorial on the Router Query Engine that helps you to set up multiple indices/ query engines for your dataset, allowing the LLM to choose the most suitable one for each specific question. \\u2699\\ufe0f Integrations & Collaborations: - We integrated the Tavily AI research API into the LlamaIndex RAG pipeline, offering a robust tool for web research to enhance LLM agent automation. Notebook, Tweet. - We integrated Noam Gat\\u2019s LLM Enforcer into the LlamaIndex RAG pipeline to ensure structured outputs for various models. Docs, Tweet. - We integrated the latest Embed v3 model from CohereAI, enhancing document retrieval quality within the LlamaIndex RAG pipeline. Notebook, Tweet. - We integrated the new Voyage AI embedding model, a top-performing option for RAG pipelines. Notebook, Tweet.\",\n          \"A few months ago, we launched LlamaIndex 0.6.0, which included a massive rewrite of our codebase to make our library more modular, customizable, and accessible to both beginner and advanced users: - We created modular storage abstractions (data, indices), and compute abstractions (retrievers, query engines). - We created a lower-level API where users could use our modules (retrievers, query engines) independently and customize it as part of a larger system. Today, we\\u2019re excited to launch LlamaIndex 0.7.0. Our latest release continues the theme of improving modularity/customizability at the lower level to enable bottoms-up development of LLM applications over your data. You now have even more control over using key abstractions: the LLM, our response synthesizer, and our Document and Node objects. - We\\u2019ve created standalone LLM abstractions (OpenAI, HuggingFace, PaLM). - We\\u2019ve made our response synthesis module an independent module you can use completely independently of the rest of our abstractions \\u2014 get rid of the prompt boilerplate of trying to figure out how to fit context within a context window. - We\\u2019ve added extensive metadata management capabilities to our Document/Node objects \\u2014 now you have complete control over context you decide to inject into your documents. Below, we describe each section more in detail. We also outline a full list of breaking changes at the bottom. Section: Standalone LLM Abstractions: We\\u2019ve created standalone LLM abstractions for OpenAI, HuggingFace, and PaLM. These abstractions can be used on their own, or as part of an existing LlamaIndex system (query engines, retrievers). Subsection: High-level Motivation: We did this for multiple reasons: - Cleaner abstractions in the codebase. Before, our LLMPredictor class had a ton of leaky abstractions with the underlying LangChain LLM class. This made our LLM abstractions hard to reason about, and hard to customize. LLMPredictor - Slightly cleaner dev UX. Before, if you wanted to customize the default LLM (for instance, use \\u201ctext-davinci-003\\u201d, you had to import the correct LangChain class, wrap it in our LLMPredictor, and then pass it to ServiceContext. Now it\\u2019s easy to just import our LLM abstraction (which is natively documented with our docs) and plug it into ServiceContext. Of course, you can still use LangChain\\u2019s LLMs if you wish. - Conducive to bottoms-up development: it makes sense to play around with these LLM modules independently before plugging them in as part of a larger system. It\\u2019s reflective of our bigger push in 0.7.0 to let users compose their own workflows. Subsection: Using on their own: Our LLM abstractions support both complete and chat endpoints. The main difference is that complete is designed to take in a simple string input, and output a CompletionResponse (containing text output + additional fields). chat takes in a ChatMessage and outputs a ChatResponse (containing a chat message + additional fields). complete chat complete CompletionResponse chat ChatMessage ChatResponse These LLM endpoints also natively support streaming via stream_complete and stream_chat. stream_complete stream_chat Here\\u2019s on how you can use the LLM abstractions on their own: Here\\u2019s how you can use the LLM abstractions as part of an overall LlamaIndex system. Note: Our top-level LLMPredictor still exists but is less user-facing (and we might deprecate in the future). Also, you can still use LangChain LLMs through our LangChainLLM class. LLMPredictor LangChainLLM Subsection: Resources: All of our notebooks have by default been updated to use our native OpenAI LLM integration. Here\\u2019s some resources to show both the LLM abstraction on its own as well as how it can be used in the overall system: - OpenAI LLM - Using LLM in LLMPredictor - Changing LLM within Index/Query Engine - Defining a custom LLM Model Section: Standalone Response Synthesis Modules: Subsection: Context: In any RAG system, there is retrieval and there is synthesis. The responsibility of the synthesis component is to take in incoming context as input, and synthesize a response using the LLM. Fundamentally, the synthesis module needs to synthesize a response over any context list, regardless of how long that context list is. This is essentially \\u201cboilerplate\\u201d that an LLM developer / \\u201cAI engineer\\u201d must write. We had this as an internal abstraction in LlamaIndex before (as a ResponseSynthesizer), but the external-facing UX was unfriendly to users. The actual piece that gathered responses (the ResponseBuilder ) was hard to customize, and the ResponseSynthesizer itself was adding an extra unnecessary layer. ResponseSynthesizer ResponseBuilder ResponseSynthesizer Now we have a set of standalone modules that you can easily import. Previously, when you set the response_mode in the query engine, these were being setup for you. Now they are more directly available and user-facing. response_mode Here\\u2019s a list of all the new Response Synthesiszer modules available from llama_index.response_synthesizer: Response Synthesiszer llama_index.response_synthesizer - Refine - Query an LLM, sending each text chunk individually. After the first LLM call, the existing answer is also sent to the LLM for updating and refinement using the next text chunk. Refine - Accumulate - Query an LLM with the same prompt across multiple text chunks, and return a formatted list of responses Accumulate - Compact - The same as Refine, but puts as much text as possible into each LLM call Compact Refine - CompactAndAccumulate - The same as Accumulate, but puts as much text as possible CompactAndAccumulate Accumulate - TreeSummarize - Create a bottom-up summary from the provided text chunks, and return the root summary TreeSummarize - SimpleSummarize - Combine and truncate all text chunks, and summarize in a single LLM call SimpleSummarize Subsection: Usage: As detailed above, you can directly set a response synthesizer in a query engine, or let the response_mode fetch the relevant response synthesizer. response_mode Furthermore though, you can directly call and use these synthesizers as low level modules. Here\\u2019s a small example: Subsection: Resources: Here are some additional notebooks showing how to use get_response_synthesizer : get_response_synthesizer - Low-level API Usage Pattern - Custom Retrievers Section: Metadata Management Capabilities: If you want to have good performance in any LLM application over your data (including a RAG pipeline), you need to make sure that your documents actually contain relevant context for the query. One way to do this is to add proper metadata, both at the document-level and after the documents have been parsed into text chunks (into Nodes). We allow you to define metadata fields within a Document, customize the ID, and also customize the metadata text/format for LLM usage and embedding usage. Defining Metadata Fields Customizing the ID The ID of each document can be set multiple ways - Within the constructor: document = Document(text=\\\"text\\\", doc_id_=\\\"id\\\") document = Document(text=\\\"text\\\", doc_id_=\\\"id\\\") - After constructing the object: document.doc_id = \\\"id\\\" document.doc_id = \\\"id\\\" - Automatically using the SimpleDirectoryReader : SimpleDirectoryReader(filename_as_id=True).load_data() SimpleDirectoryReader SimpleDirectoryReader(filename_as_id=True).load_data() Customizing the Metadata Text for LLMs and Embeddings As seen above, you can set metadata containing useful information. By default, all the metadata will be seen by the embedding model and the LLM. However, sometimes you may want to only include data to bias embeddings, or only include data as extra information for the LLM! With the new Document objects, you can configure what each metadata field is used for: Document Customizing the Metadata Format Template When the metadata is inserted into the text, it follows a very specific format. This format is configurable at multiple levels: Please check out this guide for more details! Section: Full List of Breaking Changes: Subsection: Response Synthesis + Node Postprocessors: The ResponseSynthesizer object class has been removed, and replaced with get_response_synthesizer . In addition to this, node post processors are now handled by the query engine directly, and the old SentenceEmbeddingOptimizer has been switched to become a node post processor instance itself. ResponseSynthesizer get_response_synthesizer SentenceEmbeddingOptimizer Here is an example of the required migration to use all moved features. Old New Subsection: LLM Predictor: While introducing a new LLM abstraction, we cleaned up the LLM Predictor and removed several deprecated functionalities: - Remove ChatGPTLLMPredictor and HuggingFaceLLMPredictor (use OpenAI and HuggingFaceLLM instead, see migration guide) ChatGPTLLMPredictor HuggingFaceLLMPredictor OpenAI HuggingFaceLLM - Remove support for setting cache via LLMPredictor constructor. cache LLMPredictor - Removed llama_index.token_counter.token_counter module (see migration guide). llama_index.token_counter.token_counter Now, the LLM Predictor class is mostly a lightweight wrapper on top of the LLM abstraction that handles: LLM - conversion of prompts to the string or chat message input format expected by the LLM - logging of prompts and responses to a callback manager We advice users to configure the llm argument in ServiceContext directly (instead of creating LLM Predictor). llm ServiceContext Subsection: Chat Engine: We updated the BaseChatEngine interface to take in a List[ChatMessage]] for the chat_history instead of tuple of strings. This makes the data model consistent with the input/output of the LLM , also more flexibility to specify consecutive messages with the same role. BaseChatEngine List[ChatMessage]] chat_history LLM Old New We also exposed chat_history state as a property and supported overriding chat_history in chat and achat endpoints. chat_history chat_history chat achat Subsection: Prompt Helper: We removed some previously deprecated arguments: max_input_size, embedding_limit, max_chunk_overlap max_input_size embedding_limit max_chunk_overlap Section: Conclusion: At a high-level, we hope that these changes continue to enable bottoms-up development of LLM applications over your data. We first encourage you to play around with our new modules on their own to get a sense what they do and where they can be used. Once you\\u2019re ready to use them in more advanced workflows, then you can figure out how to use our outer components to setup a sophisticated RAG pipeline. As always, our repo is here and our docs are here. If you have thoughts/comments, don\\u2019t hesitate to hop in our Discord!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1a961847-6c4b-4f08-bd35-3a0204ec24c8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>date</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/blog/introducing-llamaextract-beta-structured...</td>\n",
              "      <td>Introducing LlamaExtract Beta: structured data...</td>\n",
              "      <td>https://www.llamaindex.ai/blog/introducing-lla...</td>\n",
              "      <td>Jul 25, 2024</td>\n",
              "      <td>Structured extraction from unstructured data i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/blog/llamaindex-newsletter-2024-07-23</td>\n",
              "      <td>LlamaIndex Newsletter 2024-07-23</td>\n",
              "      <td>https://www.llamaindex.ai/blog/llamaindex-news...</td>\n",
              "      <td>Jul 23, 2024</td>\n",
              "      <td>Hello, Llama Followers! 🦙 Welcome to this week...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/blog/improving-vector-search-reranking-with-p...</td>\n",
              "      <td>Improving Vector Search - Reranking with Postg...</td>\n",
              "      <td>https://www.llamaindex.ai/blog/improving-vecto...</td>\n",
              "      <td>Jul 19, 2024</td>\n",
              "      <td>Subsection: Search and Reranking: Improving Re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/blog/the-latest-updates-to-llamacloud</td>\n",
              "      <td>The latest updates to LlamaCloud</td>\n",
              "      <td>https://www.llamaindex.ai/blog/the-latest-upda...</td>\n",
              "      <td>Jul 19, 2024</td>\n",
              "      <td>To build a production-quality LLM agent over y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/blog/case-study-how-scaleport-ai-accelerated-...</td>\n",
              "      <td>Case Study: How Scaleport.ai Accelerated Devel...</td>\n",
              "      <td>https://www.llamaindex.ai/blog/case-study-how-...</td>\n",
              "      <td>Jul 17, 2024</td>\n",
              "      <td>Subsection: The Challenge: Streamlining AI Dev...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a961847-6c4b-4f08-bd35-3a0204ec24c8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1a961847-6c4b-4f08-bd35-3a0204ec24c8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1a961847-6c4b-4f08-bd35-3a0204ec24c8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-72ff92f1-7561-4c16-a959-f67f2e60536a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-72ff92f1-7561-4c16-a959-f67f2e60536a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-72ff92f1-7561-4c16-a959-f67f2e60536a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                              source  \\\n",
              "0  /blog/introducing-llamaextract-beta-structured...   \n",
              "1             /blog/llamaindex-newsletter-2024-07-23   \n",
              "2  /blog/improving-vector-search-reranking-with-p...   \n",
              "3             /blog/the-latest-updates-to-llamacloud   \n",
              "4  /blog/case-study-how-scaleport-ai-accelerated-...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Introducing LlamaExtract Beta: structured data...   \n",
              "1                   LlamaIndex Newsletter 2024-07-23   \n",
              "2  Improving Vector Search - Reranking with Postg...   \n",
              "3                   The latest updates to LlamaCloud   \n",
              "4  Case Study: How Scaleport.ai Accelerated Devel...   \n",
              "\n",
              "                                                 url          date  \\\n",
              "0  https://www.llamaindex.ai/blog/introducing-lla...  Jul 25, 2024   \n",
              "1  https://www.llamaindex.ai/blog/llamaindex-news...  Jul 23, 2024   \n",
              "2  https://www.llamaindex.ai/blog/improving-vecto...  Jul 19, 2024   \n",
              "3  https://www.llamaindex.ai/blog/the-latest-upda...  Jul 19, 2024   \n",
              "4  https://www.llamaindex.ai/blog/case-study-how-...  Jul 17, 2024   \n",
              "\n",
              "                                             content  \n",
              "0  Structured extraction from unstructured data i...  \n",
              "1  Hello, Llama Followers! 🦙 Welcome to this week...  \n",
              "2  Subsection: Search and Reranking: Improving Re...  \n",
              "3  To build a production-quality LLM agent over y...  \n",
              "4  Subsection: The Challenge: Streamlining AI Dev...  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('llama_blog_clean.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkawQSevoalJ"
      },
      "source": [
        "<a id=\"4\"></a>\n",
        "## <div style=\"text-align: left; background-color:#DEF5B9; font-family: Trebuchet MS; color:#1D3E06; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green\">4 Chunking and Vector Embeddings 🖌️ </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n77UQb4ToalJ"
      },
      "source": [
        "<a id=\"4.1\"></a>\n",
        "### <div style=\"text-align: left; background-color:#F0DCED; font-family:Trebuchet MS;color:#8F2A46; padding: 14px; line-height: 1;border-radius:10px;border-style: solid;border-color: dark pink\">4.1 LLM Loading </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gtgtrW5BE2A",
        "outputId": "5c987dbe-2569-4b2d-cfb8-f04cde208b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face token: ··········\n"
          ]
        }
      ],
      "source": [
        "HF_TOKEN = getpass.getpass('Enter your Hugging Face token: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmvGi2vQyyEC",
        "outputId": "2d68fe2b-1146-4095-c009-ce69245327c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "edd85ec6e7064969a0627c030f20bfba",
            "a27e5d1117d34032bbcce608ad4b9eda",
            "bafe5440f9104c0fb3f9b78c80445cd5",
            "8ae10d9cf9bf4b7d93f4ec9233d85ad1",
            "10bf81ee501b4e019574c86d15610dfb",
            "52c79fb092d14659b455dbe04e081016",
            "ce9748bd2fa245d99468bc0d19068a0f",
            "14933c4a1ce94028a7ec927429c50d17",
            "8f1fb42354644bdf8758fe818d6b35f8",
            "19fbf0e1c40b4c2fa2c1db1db85bb944",
            "8cfbe2c067c242139b48bb022efad979"
          ]
        },
        "id": "YulCmzPCACyO",
        "outputId": "bf89298c-4556-4b35-804b-850026444c46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edd85ec6e7064969a0627c030f20bfba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define a function to convert messages to a prompt\n",
        "def messages_to_prompt(messages):\n",
        "  prompt = \"\"\n",
        "  for message in messages:\n",
        "    if message.role == 'system':\n",
        "      prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
        "    elif message.role == 'user':\n",
        "      prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
        "    elif message.role == 'assistant':\n",
        "      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
        "\n",
        "  # ensure we start with a system prompt, insert blank if needed\n",
        "  if not prompt.startswith(\"<|system|>\\n\"):\n",
        "    prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
        "\n",
        "  # add final assistant prompt\n",
        "  prompt = prompt + \"<|assistant|>\\n\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "# Define the quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Define the LLM\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "    tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "    query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
        "    model_kwargs={\"quantization_config\": quantization_config, \"token\": HF_TOKEN},\n",
        "    tokenizer_kwargs={\"token\": HF_TOKEN},\n",
        "    generate_kwargs={\"temperature\": 0.7, \"top_p\": 0.95},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    device_map= 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIwy97wXoalJ"
      },
      "source": [
        "<a id=\"4.2\"></a>\n",
        "### <div style=\"text-align: left; background-color:#F0DCED; font-family:Trebuchet MS;color:#8F2A46; padding: 14px; line-height: 1;border-radius:10px;border-style: solid;border-color: dark pink\">4.2 Data Loading and Chunking </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HNveVnj9p5J1"
      },
      "outputs": [],
      "source": [
        "# Create a list of Document objects from the DataFrame\n",
        "documents = [\n",
        "    Document(\n",
        "        text=row['content'],\n",
        "        metadata={\n",
        "            'source': row['source'],\n",
        "            'title': row['title'],\n",
        "            'url': row['url'],\n",
        "            'date': row['date'],\n",
        "        },\n",
        "    )\n",
        "    for index, row in df.iterrows()\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXSKOJ5EDhq0",
        "outputId": "66f82af1-a884-4e0b-c93d-37337ac0e115"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# Load embedding\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"dunzhang/stella_en_1.5B_v5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_VBRS341Ft3Q"
      },
      "outputs": [],
      "source": [
        "# Use SentenceSplitter to generate a list of nodes from documents\n",
        "nodes = SentenceSplitter(\n",
        "    chunk_size=2048,\n",
        "    chunk_overlap=256,\n",
        "    paragraph_separator=\"\\n\\n\",\n",
        ").get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVAckGwr6FHV",
        "outputId": "e3eb447e-320c-4f93-f670-6d520a6cb721"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextNode(id_='3c75b653-cf80-4370-a505-3b8d5aa51347', embedding=None, metadata={'source': '/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks', 'title': 'Introducing LlamaExtract Beta: structured data extraction in just a few clicks', 'url': 'https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks', 'date': 'Jul 25, 2024'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2577e04-9ee3-4e8f-805f-def3af8ffe43', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': '/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks', 'title': 'Introducing LlamaExtract Beta: structured data extraction in just a few clicks', 'url': 'https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks', 'date': 'Jul 25, 2024'}, hash='b2dd97f36af5cddd37eb79eba51be76e14c88501b1166bbfd6361bc2bfb3f874')}, text='Structured extraction from unstructured data is both a core use case for LLMs in its own right, as well as a key ingredient in data processing for retrieval and RAG use cases. Today we’re excited to announce a beta release of LlamaExtract, which is a managed service that lets you perform structured extraction from unstructured documents. It does the following: - Infers a schema from an existing candidate set of documents. You can choose to edit this schema later. - Extracts values from a set of documents according to a specified schema (whether inferred from the previous step, specified by a human, or both). LlamaExtract is available to LlamaCloud users through both a UI and API. Schema inference currently comes with a cap of 5 files, with a max of 10 pages per file. Given an existing schema, schema extraction happens on a per-document level. LlamaExtract is currently in beta, meaning it is an experimental feature that we’re working hard to improve and make more generally scalable and usable. Please report any issues to our Github! beta, Subsection: Metadata Extraction is a Key Part of the LLM ETL Stack: A new data ETL stack is needed for LLM applications. This data loading, transformation, and indexing layer is crucial for downstream RAG and agent use cases over unstructured data. We built LlamaParse and LlamaCloud to serve these ETL needs and power thousands of production pipelines over complex documents in production. Through working with our users and customers, we realized that besides chunk-level embeddings, automated metadata extraction is an important part of the transformation story (the “T” in ETL); it is a core necessary ingredient for increasing transparency and control over broad swaths of unstructured data. That led us to build the initial version of LlamaExtract - designed to automate data transformation for your unstructured data. Subsection: A Walkthrough: LlamaExtract is an API, it also has a python client and of course a web UI within LlamaCloud. Use the UI to Prototype The LlamaExtract UI allows you to prototype an extraction job. After clicking the “Extraction (beta)” tab in the left-hand side, you can click “Create a new schema” in order to define a new extraction job, which will take you to the schema creation screen: Drag and drop your files into the UI. Click “Next” to kick off schema inference - the inferred JSON schema will be shown in the “Schema” section below: This follows the Pydantic JSON schema format; after inference you have full freedom to customize and modify the inferred schema. The schema visualization will reflect your changes. Once you’re happy with your schema, you can kick off extraction, which will return the final JSON object with extracted keys and values in adherence with the schema. There are some core improvements we are making to the UI, like decoupling schema inference and extraction, allowing users to pre-define a schema, etc. For more flexibility check out the API below. Use the API to Create Extraction Workflows The API allows users to more flexibly integrate schema inference and extraction. To access the API via our client package, follow these steps: pip install llama-extract You can choose to either infer a schema or specify your own schema (or infer first, and then modify it programmatically after to your liking). If you wish to use LlamaExtract’s schema inference capability, do: infer from llama_extract import LlamaExtract extractor = LlamaExtract() extraction_schema = extractor.infer_schema(\"Test Schema\", [\"./file1.pdf\",\"./file2.pdf\"]) If you prefer you can specify the schema directly rather than inferring it. The easiest way is to define a Pydantic object and convert that to a JSON schema: from pydantic import BaseModel, Field class ResumeMetadata(BaseModel): \"\"\"Resume metadata.\"\"\" years_of_experience: int = Field(..., description=\"Number of years of work experience.\") highest_degree: str = Field(..., description=\"Highest degree earned (options: High School, Bachelor\\'s, Master\\'s, Doctoral, Professional\") professional_summary: str = Field(..., description=\"A general summary of the candidate\\'s experience\") extraction_schema = extractor.create_schema(\"Test Schema\", ResumeMetadata) However you get your schema, you can now perform extraction: extractions = extractor.extract(extraction_schema.id, [\"./file3.pdf\",\"./file4.pdf\"]) You can see the extracted data: print(extractions[0].data) Subsection: Some Example Use Cases: LlamaExtract uses LlamaParse as its underlying parser and is able to handle complex document types (note: native multimodal extraction coming soon!) Through our initial explorations, here are some initial datasets where LlamaExtract is valuable: - Resumes: extract structured annotations like school, work experiences, YOE from a candidate’s profile - Receipts and Invoices: extract line items, total price, and other figures. - Product pages: structure and categorize your products according to a user-defined schema. Check out the examples section of the LlamaExtract client repo to see more possibilities. Subsection: We’re Rapidly Improving our Extraction Capabilities: By opening this up to the community, we’re excited to rapidly improve the UX, scalability, and performance. Here’s an feature roadmap: - Multimodal extraction - Decouple schema creation from extraction in the UI - Support for human-in-the-loop schema creation as a first class UX - Make automated schema inference and extraction more robust over longer documents (e.g. a 10K filing) Subsection: Try it Out: LlamaExtract is available to all users. You don’t need to be let off a waitlist! Just create an account at cloud.llamaindex.ai to use the UI; you can also check out our python client. To get started, try one of these notebooks: all - LlamaExtract quick start - LlamaExtract with Pydantic - LlamaExtract + RAG If you’re interested in the broader LlamaCloud functionality, join our waitlist.', mimetype='text/plain', start_char_idx=0, end_char_idx=5927, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nodes[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PXEYJLTAU-gD"
      },
      "outputs": [],
      "source": [
        "# Create a SimpleDocumentStore and add the documents to it\n",
        "docstore = SimpleDocumentStore()\n",
        "docstore.add_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc4yIqG5oalL"
      },
      "source": [
        "<a id=\"4.3\"></a>\n",
        "### <div style=\"text-align: left; background-color:#F0DCED; font-family:Trebuchet MS;color:#8F2A46; padding: 14px; line-height: 1;border-radius:10px;border-style: solid;border-color: dark pink\">4.3 Data Embedding and Vector Store </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCNrP55ZZFM-",
        "outputId": "557e7007-c6f4-4c7d-9c1c-fc1f23bc7333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Qdrant API key: ··········\n",
            "collections=[]\n"
          ]
        }
      ],
      "source": [
        "# Prompt the user to enter their Qdrant API key\n",
        "QDRANT_API_KEY = getpass.getpass('Enter your Qdrant API key: ')\n",
        "QDRANT_URL = \"https://ed3265f8-b409-4cff-9259-4d2ebbc2cc93.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
        "QDRANT_COLLECTION_NAME = \"llama-index_collection\"\n",
        "\n",
        "# Initialize the Qdrant client with the provided URL and API key\n",
        "qdrant_client = QdrantClient(\n",
        "    url=QDRANT_URL,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Attempt to delete the specified collection, ignore any errors\n",
        "try:\n",
        "  qdrant_client.delete_collection(QDRANT_COLLECTION_NAME)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "print(qdrant_client.get_collections())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgBtt1MIVL8k",
        "outputId": "4d39e4c0-1acc-48ca-c0b0-22c863359f57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        }
      ],
      "source": [
        "# Initialize a QdrantVectorStore with the Qdrant client and collection name\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=qdrant_client,\n",
        "    collection_name=QDRANT_COLLECTION_NAME,\n",
        ")\n",
        "\n",
        "# Create a StorageContext using the vector store\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Create a VectorStoreIndex with the nodes and storage context\n",
        "index = VectorStoreIndex(nodes, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_67XTy7LamJX",
        "outputId": "c7d67e27-8986-4680-f41d-1274907b3492"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "collections=[CollectionDescription(name='llama-index_collection')]\n"
          ]
        }
      ],
      "source": [
        "print(qdrant_client.get_collections())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ia1kdAmA2ON",
        "outputId": "8a1b7170-fd57-42b0-bf74-4d2fffaaf3ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CountResult(count=206)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qdrant_client.count(\n",
        "    collection_name=QDRANT_COLLECTION_NAME,\n",
        "    exact=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eQN51G-oalR"
      },
      "source": [
        "<a id=\"5\"></a>\n",
        "## <div style=\"text-align: left; background-color:#DEF5B9; font-family: Trebuchet MS; color:#1D3E06; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green\">5. Query Engine </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO-4m3jfoalS"
      },
      "source": [
        "<a id=\"5.1\"></a>\n",
        "### <div style=\"text-align: left; background-color:#F0DCED; font-family:Trebuchet MS;color:#8F2A46; padding: 14px; line-height: 1;border-radius:10px;border-style: solid;border-color: dark pink\">5.1 Public Testcases with QueryEngineTool</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Gng5kQlttxNv"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "# from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "\n",
        "# rerank = SentenceTransformerRerank(\n",
        "#     model=\"dunzhang/stella_en_1.5B_v5\", top_n=3\n",
        "# )\n",
        "# ERROR: Colab disk space having full\n",
        "\n",
        "# Create a QueryEngineTool with the index\n",
        "query_engine = index.as_query_engine(\n",
        "    # node_post_processor=[rerank]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "9XmJVwOJoLHR",
        "outputId": "cc4e54c7-7d3f-4e6c-c56a-f1c909258e80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** llama-agents are a type of RAG (Retrieval-as-Generation) application that uses LLM (Language Learning Model) to generate responses to user queries by retrieving and synthesizing relevant information from a large corpus of text. They are designed to provide accurate and relevant responses to complex and open-ended questions, and can handle a wide range of data sources and formats. Some key features of llama-agents include:\n",
              "\n",
              "1. Retrieval: llama-agents use a retrieval system to identify the most relevant documents or passages for a given query. This can involve techniques such as vector search, keyword search, or full-text search.\n",
              "\n",
              "2. Generation: llama-agents use an LLM to generate a response to the user query based on the retrieved information. This can involve techniques such as chain-of-thought reasoning, summarization, or question answering.\n",
              "\n",
              "3. Context: llama-agents can handle large context windows, allowing for more accurate and relevant responses to complex and open-ended questions.\n",
              "\n",
              "4. Personalization: llama-agents can learn from user interactions and preferences, allowing for more personalized and relevant responses over time"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source: /blog/one-click-open-source-rag-observability-with-langfuse\n",
            "Title: One-click Open Source RAG Observability with Langfuse\n",
            "URL: https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse\n",
            "Date: Mar 18, 2024\n",
            "Elapsed: 27.22s\n"
          ]
        }
      ],
      "source": [
        "now = time()\n",
        "response = query_engine.query(\"What are key features of llama-agents?\")\n",
        "display_response(response)\n",
        "\n",
        "print(f\"Source: {response.source_nodes[0].metadata['source']}\")\n",
        "print(f\"Title: {response.source_nodes[0].metadata['title']}\")\n",
        "print(f\"URL: {response.source_nodes[0].metadata['url']}\")\n",
        "print(f\"Date: {response.source_nodes[0].metadata['date']}\")\n",
        "print(f\"Elapsed: {round(time() - now, 2)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "9TLc12SV1XSb",
        "outputId": "0a10bbaf-f0c7-4f46-87b1-111cef77fe9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook are the Retrieval System and Response Generation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source: /blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n",
            "Title: Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex\n",
            "URL: https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n",
            "Date: Oct 5, 2023\n",
            "Elapsed: 8.41s\n"
          ]
        }
      ],
      "source": [
        "now = time()\n",
        "response = query_engine.query(\"\"\"What are the two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook?\"\"\")\n",
        "display_response(response)\n",
        "\n",
        "print(f\"Source: {response.source_nodes[0].metadata['source']}\")\n",
        "print(f\"Title: {response.source_nodes[0].metadata['title']}\")\n",
        "print(f\"URL: {response.source_nodes[0].metadata['url']}\")\n",
        "print(f\"Date: {response.source_nodes[0].metadata['date']}\")\n",
        "print(f\"Elapsed: {round(time() - now, 2)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "HFC-wCU7vrTW",
        "outputId": "8b78efda-bc85-41b9-b84a-2c383f535361"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The two main metrics used to evaluate the performance of the different rerankers in the RAG system are hit rate and MRR (mean reciprocal rank). These metrics are discussed in the context of the benchmarks presented in the blog post \"Boosting RAG: Picking the Best Embedding & Reranker models\" (https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source: /blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\n",
            "Title: Boosting RAG: Picking the Best Embedding & Reranker models\n",
            "URL: https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\n",
            "Date: Nov 3, 2023\n",
            "Elapsed: 15.14s\n"
          ]
        }
      ],
      "source": [
        "now = time()\n",
        "response = query_engine.query(\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\")\n",
        "display_response(response)\n",
        "\n",
        "print(f\"Source: {response.source_nodes[0].metadata['source']}\")\n",
        "print(f\"Title: {response.source_nodes[0].metadata['title']}\")\n",
        "print(f\"URL: {response.source_nodes[0].metadata['url']}\")\n",
        "print(f\"Date: {response.source_nodes[0].metadata['date']}\")\n",
        "print(f\"Elapsed: {round(time() - now, 2)}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR7gBFt4oalS"
      },
      "source": [
        "<a id=\"5.2\"></a>\n",
        "### <div style=\"text-align: left; background-color:#F0DCED; font-family:Trebuchet MS;color:#8F2A46; padding: 14px; line-height: 1;border-radius:10px;border-style: solid;border-color: dark pink\">5.2 Router Query Engine </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "0h2_KAV3nrE3"
      },
      "outputs": [],
      "source": [
        "# Create a QueryEngineTool for vector search and summary\n",
        "vector_tool = QueryEngineTool(\n",
        "    index.as_query_engine(),\n",
        "    metadata=ToolMetadata(\n",
        "        name=\"vector_search\",\n",
        "        description=\"Useful for searching for specific facts.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "summary_tool = QueryEngineTool(\n",
        "    index.as_query_engine(response_mode = \"tree_summarize\"),\n",
        "    metadata=ToolMetadata(\n",
        "        name=\"summary\",\n",
        "        description=\"Useful for getting a summary of the document.\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "r-7ZxWRBCDv-",
        "outputId": "2aa29052-93ce-428a-b1d7-87c3a679e14f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: To find specific facts about llama-agents, such as their memory capacity, processing speed, or training methods, choice 1 would be the most relevant..\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** llama-agents are a type of RAG (Retrieval-as-Generation) application that uses LLM (Language Learning Model) to generate responses to user queries by retrieving and synthesizing relevant information from a large corpus of text. They are designed to provide accurate and relevant responses to complex and open-ended questions, and can handle a wide range of data sources and formats. Some key features of llama-agents include:\n",
              "\n",
              "1. Retrieval: llama-agents use a retrieval system to identify the most relevant documents or passages for a given query. This can involve techniques such as vector search, keyword search, or full-text search.\n",
              "\n",
              "2. Generation: llama-agents use an LLM to generate a response to the user query based on the retrieved information. This can involve techniques such as chain-of-thought reasoning, summarization, or question answering.\n",
              "\n",
              "3. Context: llama-agents can handle large context windows, allowing for more accurate and relevant responses to complex and open-ended questions.\n",
              "\n",
              "4. Personalization: llama-agents can learn from user interactions and preferences, allowing for more personalized and relevant responses over time"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "\n",
        "# Create a RouterQueryEngine with the vector and summary tools\n",
        "query_engine = RouterQueryEngine.from_defaults(\n",
        "    [vector_tool, summary_tool],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Query the engine with a question\n",
        "response = query_engine.query(\"What are key features of llama-agents?\")\n",
        "display_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV6kwkC3oalT"
      },
      "source": [
        "<a id=\"5.3\"></a>\n",
        "### <div style=\"text-align: left; background-color:#F0DCED; font-family:Trebuchet MS;color:#8F2A46; padding: 14px; line-height: 1;border-radius:10px;border-style: solid;border-color: dark pink\">5.3 User Interface Demo </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "39_2N40-KQIZ",
        "outputId": "a9603225-d0fc-4d9e-8037-f5cf88720d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt: What are key features of llama-agents?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** llama-agents are a type of RAG (Retrieval-as-Generation) application that uses LLM (Language Learning Model) to generate responses to user queries by retrieving and synthesizing relevant information from a large corpus of text. They are designed to provide accurate and relevant responses to complex and open-ended questions, and can handle a wide range of data sources and formats. Some key features of llama-agents include:\n",
              "\n",
              "1. Retrieval: llama-agents use a retrieval system to identify the most relevant documents or passages for a given query. This can involve techniques such as vector search, keyword search, or full-text search.\n",
              "\n",
              "2. Generation: llama-agents use an LLM to generate a response to the user query based on the retrieved information. This can involve techniques such as chain-of-thought reasoning, summarization, or question answering.\n",
              "\n",
              "3. Context: llama-agents can handle large context windows, allowing for more accurate and relevant responses to complex and open-ended questions.\n",
              "\n",
              "4. Personalization: llama-agents can learn from user interactions and preferences, allowing for more personalized and relevant responses over time"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Final Response: ** llama-agents are a type of RAG (Retrieval-as-Generation) application that uses LLM (Language Learning Model) to generate responses to user queries by retrieving and synthesizing relevant information from a large corpus of text. They are designed to provide accurate and relevant responses to complex and open-ended questions, and can handle a wide range of data sources and formats. Some key features of llama-agents include:\n",
            "\n",
            "1. Retrieval: llama-agents use a retrieval system to identify the most relevant documents or passages for a given query. This can involve techniques such as vector search, keyword search, or full-text search.\n",
            "\n",
            "2. Generation: llama-agents use an LLM to generate a response to the user query based on the retrieved information. This can involve techniques such as chain-of-thought reasoning, summarization, or question answering.\n",
            "\n",
            "3. Context: llama-agents can handle large context windows, allowing for more accurate and relevant responses to complex and open-ended questions.\n",
            "\n",
            "4. Personalization: llama-agents can learn from user interactions and preferences, allowing for more personalized and relevant responses over time\n",
            "Source: /blog/one-click-open-source-rag-observability-with-langfuse\n",
            "Title: One-click Open Source RAG Observability with Langfuse\n",
            "URL: https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse\n",
            "Date: Mar 18, 2024\n",
            "Elapsed: 27.37s\n"
          ]
        }
      ],
      "source": [
        "# Prompt the user to enter their input\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "\n",
        "# Query the engine with the prompt\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "now = time()\n",
        "response = query_engine.query(prompt)\n",
        "display_response(response)\n",
        "\n",
        "print(f\"**Final Response: ** {response.response}\")\n",
        "print(f\"Source: {response.source_nodes[0].metadata['source']}\")\n",
        "print(f\"Title: {response.source_nodes[0].metadata['title']}\")\n",
        "print(f\"URL: {response.source_nodes[0].metadata['url']}\")\n",
        "print(f\"Date: {response.source_nodes[0].metadata['date']}\")\n",
        "print(f\"Elapsed: {round(time() - now, 2)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "A_4DOOT2r3Yp"
      },
      "outputs": [],
      "source": [
        "# Create a RouterQueryEngine with the vector and summary tools\n",
        "query_engine = RouterQueryEngine.from_defaults(\n",
        "    [vector_tool, summary_tool],\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "A2mDYZMVp_nm",
        "outputId": "998c7ddc-a9b6-440b-cdee-cb39e48b2286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://7fbead98693bf7ecbe.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://7fbead98693bf7ecbe.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define a function to query the RAG system\n",
        "def query_rag_system(question):\n",
        "    response = query_engine.query(question).response\n",
        "    return response\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(fn=query_rag_system,\n",
        "                     inputs=\"text\",\n",
        "                     outputs=\"text\",\n",
        "                     title=\"LlamaIndex Chatbot\",\n",
        "                     description=\"Ask any question about LlamaIndex.\")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(share = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxOa_brCoalT"
      },
      "source": [
        "<a id=\"6\"></a>\n",
        "## <div style=\"text-align: left; background-color:#DEF5B9; font-family: Trebuchet MS; color:#1D3E06; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green\">6. Conclusion </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xes5sfwoalT"
      },
      "source": [
        "<div style=\"border-radius:10px;\n",
        "            border :#0A0104 solid;\n",
        "            padding: 15px;\n",
        "            font-size:110%;\n",
        "            text-align: left\">\n",
        "This project successfully implemented a RAG-based question-answering system using LlamaIndex. The system shows promise in providing accurate and contextual responses to queries about LlamaIndex."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10bf81ee501b4e019574c86d15610dfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14933c4a1ce94028a7ec927429c50d17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19fbf0e1c40b4c2fa2c1db1db85bb944": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52c79fb092d14659b455dbe04e081016": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ae10d9cf9bf4b7d93f4ec9233d85ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19fbf0e1c40b4c2fa2c1db1db85bb944",
            "placeholder": "​",
            "style": "IPY_MODEL_8cfbe2c067c242139b48bb022efad979",
            "value": " 8/8 [01:07&lt;00:00,  7.56s/it]"
          }
        },
        "8cfbe2c067c242139b48bb022efad979": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f1fb42354644bdf8758fe818d6b35f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a27e5d1117d34032bbcce608ad4b9eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52c79fb092d14659b455dbe04e081016",
            "placeholder": "​",
            "style": "IPY_MODEL_ce9748bd2fa245d99468bc0d19068a0f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "bafe5440f9104c0fb3f9b78c80445cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14933c4a1ce94028a7ec927429c50d17",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f1fb42354644bdf8758fe818d6b35f8",
            "value": 8
          }
        },
        "ce9748bd2fa245d99468bc0d19068a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edd85ec6e7064969a0627c030f20bfba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a27e5d1117d34032bbcce608ad4b9eda",
              "IPY_MODEL_bafe5440f9104c0fb3f9b78c80445cd5",
              "IPY_MODEL_8ae10d9cf9bf4b7d93f4ec9233d85ad1"
            ],
            "layout": "IPY_MODEL_10bf81ee501b4e019574c86d15610dfb"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
